{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b15aa730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from scipy.signal import peak_prominences\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623fb853",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"/Users/scottmerrill/Documents/UNC/MultiModal/VMR/Youtube8m\"\n",
    "filenames = os.listdir(path + '/video')\n",
    "file_name = filenames[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02ecae",
   "metadata": {},
   "source": [
    "### 1.  DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a91e4388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoAudioDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.filenames = os.listdir(os.path.join(path, 'video'))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        video_data = np.load(os.path.join(self.path, 'video', filename))\n",
    "        audio_data = np.load(os.path.join(self.path, 'audio', filename))\n",
    "        video_data = video_data[:, :1024]\n",
    "        return video_data, audio_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60269016",
   "metadata": {},
   "source": [
    "### 2. Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8455610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim=1024, embed_dim=512, num_heads=8, num_layers=2, max_seq_len=50):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, embed_dim)  # Project input to embedding dim\n",
    "        self.pos_encoder = self._generate_sinusoidal_positional_encoding(max_seq_len, embed_dim)\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads), num_layers=num_layers)\n",
    "        self.output_proj = nn.Linear(embed_dim, embed_dim)  # Project to final embedding\n",
    "\n",
    "    def _generate_sinusoidal_positional_encoding(self, max_len, embed_dim):\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(math.log(10000.0) / embed_dim))\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # Shape: (1, max_len, embed_dim)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_proj(x)  # Shape: (seq_len, embed_dim)\n",
    "        seq_len = x.size(0)\n",
    "        x = x + self.pos_encoder[:, :seq_len, :].squeeze(0).to(x.device)\n",
    "        x = self.transformer(x.unsqueeze(1), src_key_padding_mask=mask).squeeze(1)\n",
    "        x = x.mean(dim=0)  # Aggregate sequence to fixed-size embedding\n",
    "        return self.output_proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13aa8cc",
   "metadata": {},
   "source": [
    "### Optical Flow Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dabc43ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpticalFlowProcessor:\n",
    "    def __init__(self, method='video', window_size=20, max_segments=10, min_frames=10):\n",
    "        self.method = method\n",
    "        self.window_size = window_size\n",
    "        self.max_segments = max_segments\n",
    "        self.min_frames = min_frames\n",
    "\n",
    "    def get_best_worst_flow(self, rgb, audio):\n",
    "        flow = self._compute_flow(rgb, audio)\n",
    "        segments = self._optical_flow_segments(flow)\n",
    "        ranks = self._rank_averages(self._compute_segment_means(segments, flow))\n",
    "        return self._extract_best_worst_segments(segments, ranks)\n",
    "\n",
    "    def _compute_flow(self, rgb, audio):\n",
    "        if self.method == 'video':\n",
    "            return self._moving_average(self._calculate_optical_flow_euclidean(rgb))\n",
    "        elif self.method == 'audio':\n",
    "            return self._moving_average(self._calculate_optical_flow_euclidean(audio))\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'video' or 'audio'\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_optical_flow_euclidean(embedding_seq):\n",
    "        return np.linalg.norm(embedding_seq[1:] - embedding_seq[:-1], axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _moving_average(arr, window_size=5):\n",
    "        return np.convolve(arr, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "    def _optical_flow_segments(self, optical_flow):\n",
    "        peaks, _ = find_peaks(optical_flow)\n",
    "        prominences = peak_prominences(optical_flow, peaks)[0]\n",
    "        peak_index = peaks[np.argsort(prominences)[-self.max_segments:]]\n",
    "        peak_index = self._merge_intervals(np.sort(peak_index))\n",
    "        return np.insert(np.append(peak_index, len(optical_flow)), 0, 0)\n",
    "\n",
    "    def _merge_intervals(self, arr):\n",
    "        merged = [arr[0]]\n",
    "        for i in range(1, len(arr)):\n",
    "            if arr[i] - merged[-1] >= self.min_frames:\n",
    "                merged.append(arr[i])\n",
    "        return np.array(merged)\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_segment_means(segments, values):\n",
    "        return [values[start:end].mean() if start < end else 0 for start, end in zip(segments[:-1], segments[1:])]\n",
    "\n",
    "    @staticmethod\n",
    "    def _rank_averages(averages):\n",
    "        sorted_indices = np.argsort(averages)[::-1]\n",
    "        ranks = np.zeros_like(sorted_indices) + 1\n",
    "        for rank, idx in enumerate(sorted_indices):\n",
    "            ranks[idx] = rank + 1\n",
    "        return ranks\n",
    "\n",
    "    def _extract_best_worst_segments(self, segments, ranks):\n",
    "        top_start, top_end = segments[np.where(ranks == 1)[0][0]], segments[np.where(ranks == 1)[0][0] + 1]\n",
    "        bottom_start, bottom_end = segments[np.where(ranks == max(ranks))[0][0]], segments[np.where(ranks == max(ranks))[0][0] + 1]\n",
    "        return (top_start, top_end), (bottom_start, bottom_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c312d3d",
   "metadata": {},
   "source": [
    "### Script Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee01b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, processor):\n",
    "    video_batch, audio_batch = zip(*batch)\n",
    "    video_batch = [torch.tensor(v, dtype=torch.float32) for v in video_batch]\n",
    "    audio_batch = [torch.tensor(a, dtype=torch.float32) for a in audio_batch]\n",
    "    flow_ranks = [processor.get_best_worst_flow(video_batch[i], audio_batch[i]) for i in range(len(video_batch))]\n",
    "    return video_batch, audio_batch, flow_ranks\n",
    "\n",
    "def get_dataloader(path, batch_size=32, shuffle=True, method='video', window_size=20):\n",
    "    dataset = VideoAudioDataset(path)\n",
    "    processor = OpticalFlowProcessor(method=method, window_size=window_size)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=lambda batch: collate_fn(batch, processor))\n",
    "\n",
    "def perform_feature_padding(video_features, audio_features, start_segment, end_segment, max_seq_len):\n",
    "    vf = torch.tensor(video_features[start_segment:end_segment,:])\n",
    "    af = torch.tensor(audio_features[start_segment:end_segment,:])\n",
    "\n",
    "    pvf = torch.zeros(max_seq_len, 1024)\n",
    "    pvf[:vf.shape[0], :] = vf\n",
    "\n",
    "    paf = torch.zeros(max_seq_len, 128)\n",
    "    paf[:af.shape[0], :] = af\n",
    "\n",
    "    # Create mask (True for padding positions)\n",
    "    mask = torch.arange(max_seq_len) >= vf.shape[0]\n",
    "    mask = mask.unsqueeze(0)  # Convert to 2D (batch_size=1, seq_len)\n",
    "    return pvf, paf, mask\n",
    "\n",
    "# Function to find pairs with approximately equal differences\n",
    "def find_matching_index_pairs(array1, array2, tolerance=5):\n",
    "    # Calculate differences in array1 and array2\n",
    "    array1_diffs = np.diff(array1)\n",
    "    array2_diffs = np.diff(array2)\n",
    "\n",
    "    matching_pairs = []\n",
    "\n",
    "    # Loop through differences in array1\n",
    "    for i, diff1 in enumerate(array1_diffs):\n",
    "        # Find pairs of consecutive indices in array2 with similar differences\n",
    "        for j, diff2 in enumerate(array2_diffs):\n",
    "            if abs(diff1 - diff2) <= tolerance:  # If the difference is within the tolerance\n",
    "                matching_pairs.append(((i, i + 1), (j, j + 1), diff1, diff2))\n",
    "\n",
    "    return matching_pairs\n",
    "\n",
    "\n",
    "def get_similar_length_segments(positive_segments, negative_segments, tolerance = 5):\n",
    "    \n",
    "    while True:\n",
    "        matching_indexes = find_matching_index_pairs(positive_segments, negative_segments, tolerance=tolerance)\n",
    "        tolerance += 5\n",
    "        if len(matching_indexes) > 0:\n",
    "            break\n",
    "            \n",
    "    # sample randomly for all segments within the tolerance band\n",
    "    pos_segment, negative_segment, pos_time, neg_time = matching_indexes[np.random.randint(0, len(matching_indexes))]\n",
    "    \n",
    "    return pos_segment, negative_segment, pos_time, neg_time\n",
    "\n",
    "def get_positive_negative_embeddings(filenames, tolerance=5):\n",
    "\n",
    "    positive_record_file = np.random.choice(filenames)\n",
    "    negative_record_file = np.random.choice(filenames)\n",
    "\n",
    "    positive_rgb, positive_audio = sample_dataset(positive_record_file)\n",
    "    negative_rgb, negative_audio = sample_dataset(negative_record_file)\n",
    "\n",
    "    optical_flow_pos = calculate_optical_flow_euclidean(positive_rgb)\n",
    "    optical_flow_pos = moving_average(optical_flow_pos, window_size=20)\n",
    "\n",
    "    optical_flow_neg = calculate_optical_flow_euclidean(negative_rgb)\n",
    "    optical_flow_neg = moving_average(optical_flow_neg, window_size=20)\n",
    "\n",
    "    positive_segments = optical_flow_segments(optical_flow_pos)\n",
    "    negative_segments = optical_flow_segments(optical_flow_neg)\n",
    "\n",
    "    pos_segment, negative_segment, pos_time, neg_time = get_similar_length_segments(positive_segments, negative_segments, tolerance = 5)\n",
    "\n",
    "    # converting segment index to time in seconds\n",
    "    pos_start, pos_end = pos_segment\n",
    "    pos_start = positive_segments[pos_start]\n",
    "    pos_end = positive_segments[pos_end]\n",
    "    print(pos_start, pos_end)\n",
    "\n",
    "    # retrieving segment and padding it appropriately\n",
    "    pos_video, pos_audio, pos_mask = perform_feature_padding(positive_rgb, positive_audio, pos_start, pos_end, max_seq_len)\n",
    "\n",
    "    # computing embedding\n",
    "    positive_video_embedding = video_model(pos_video, pos_mask)\n",
    "    positive_audio_embedding = audio_model(pos_audio, pos_mask)\n",
    "\n",
    "    # converting segment index to time in seconds\n",
    "    neg_start, neg_end = negative_segment\n",
    "    neg_start = negative_segments[neg_start]\n",
    "    neg_end = negative_segments[neg_end]\n",
    "\n",
    "    # retrieving segment and padding it appropriately\n",
    "    neg_video, neg_audio, neg_mask = perform_feature_padding(negative_rgb, negative_audio, neg_start, neg_end, max_seq_len)\n",
    "\n",
    "    # computing embedding\n",
    "    negative_video_embedding = video_model(neg_video, neg_mask)\n",
    "    negative_audio_embedding = audio_model(neg_audio, neg_mask)\n",
    "    return positive_video_embedding, positive_audio_embedding, negative_video_embedding, negative_audio_embedding\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, filename):\n",
    "    \"\"\"Saves model and optimizer state dict.\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved at epoch {epoch} to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e511c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positive_negative_embeddings(video_batch, tolerance=5):\n",
    "\n",
    "    positive_record_file = np.random.choice(filenames)\n",
    "    negative_record_file = np.random.choice(filenames)\n",
    "\n",
    "    positive_rgb, positive_audio = sample_dataset(positive_record_file)\n",
    "    negative_rgb, negative_audio = sample_dataset(negative_record_file)\n",
    "\n",
    "    optical_flow_pos = calculate_optical_flow_euclidean(positive_rgb)\n",
    "    optical_flow_pos = moving_average(optical_flow_pos, window_size=20)\n",
    "\n",
    "    optical_flow_neg = calculate_optical_flow_euclidean(negative_rgb)\n",
    "    optical_flow_neg = moving_average(optical_flow_neg, window_size=20)\n",
    "\n",
    "    positive_segments = optical_flow_segments(optical_flow_pos)\n",
    "    negative_segments = optical_flow_segments(optical_flow_neg)\n",
    "\n",
    "    pos_segment, negative_segment, pos_time, neg_time = get_similar_length_segments(positive_segments, negative_segments, tolerance = 5)\n",
    "\n",
    "    # converting segment index to time in seconds\n",
    "    pos_start, pos_end = pos_segment\n",
    "    pos_start = positive_segments[pos_start]\n",
    "    pos_end = positive_segments[pos_end]\n",
    "    print(pos_start, pos_end)\n",
    "\n",
    "    # retrieving segment and padding it appropriately\n",
    "    pos_video, pos_audio, pos_mask = perform_feature_padding(positive_rgb, positive_audio, pos_start, pos_end, max_seq_len)\n",
    "\n",
    "    # computing embedding\n",
    "    positive_video_embedding = video_model(pos_video, pos_mask)\n",
    "    positive_audio_embedding = audio_model(pos_audio, pos_mask)\n",
    "\n",
    "    # converting segment index to time in seconds\n",
    "    neg_start, neg_end = negative_segment\n",
    "    neg_start = negative_segments[neg_start]\n",
    "    neg_end = negative_segments[neg_end]\n",
    "\n",
    "    # retrieving segment and padding it appropriately\n",
    "    neg_video, neg_audio, neg_mask = perform_feature_padding(negative_rgb, negative_audio, neg_start, neg_end, max_seq_len)\n",
    "\n",
    "    # computing embedding\n",
    "    negative_video_embedding = video_model(neg_video, neg_mask)\n",
    "    negative_audio_embedding = audio_model(neg_audio, neg_mask)\n",
    "    return positive_video_embedding, positive_audio_embedding, negative_video_embedding, negative_audio_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "af74f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 100\n",
    "\n",
    "\n",
    "def get_segmentd_embeddings(video_model, audio_model, vid, aud):\n",
    "    vid_segment_embeddings = []\n",
    "    \n",
    "\n",
    "    of = OpticalFlowProcessor()\n",
    "    flow = of._compute_flow(vid, aud)\n",
    "    segments = of._optical_flow_segments(flow)\n",
    "\n",
    "    vid_segment_embeddings = []\n",
    "    aud_segment_embeddings = []\n",
    "    for i in range(1, len(segments)):\n",
    "        start = segments[i-1]\n",
    "        end = segments[i]\n",
    "\n",
    "        vid_emb, aud_emb, mask = perform_feature_padding(vid, aud, start, end, max_seq_len)\n",
    "        \n",
    "        vid_segment_embeddings.append(video_model(vid_emb, mask))\n",
    "        aud_segment_embeddings.append(audio_model(aud_emb, mask))\n",
    "    return vid_segment_embeddings, aud_segment_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6ac618a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottmerrill/opt/anaconda3/envs/coingame/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "num_heads =1\n",
    "num_layers=1\n",
    "audio_model = Transformer(input_dim=128, embed_dim=256, num_heads=num_heads, num_layers=num_layers, max_seq_len=max_seq_len)\n",
    "video_model = Transformer(input_dim=1024, embed_dim=256, num_heads=num_heads, num_layers=num_layers, max_seq_len=max_seq_len)\n",
    "\n",
    "def get_batch_embeddings(video_model, audio_model, video_batch, audio_batch):\n",
    "    # We precompute the segment embeddings in each batch.  We do this once and then proceed to processing batch\n",
    "    batch_vid_embeddings = []\n",
    "    batch_aud_embeddings = []\n",
    "    for i in range(len(video_batch)):\n",
    "        vid = video_batch[i]\n",
    "        aud = audio_batch[i]\n",
    "        vid_sgmt_emb, aud_sgmt_emb = get_segmentd_embeddings(video_model, audio_model, vid, aud)\n",
    "        batch_vid_embeddings.extend(vid_sgmt_emb)\n",
    "        batch_aud_embeddings.extend(aud_sgmt_emb)\n",
    "    # Shape will by (total segments X embedding dim)\n",
    "    # total segments is clip dependent\n",
    "    batch_aud_embeddings = torch.stack(batch_aud_embeddings)\n",
    "    batch_vid_embeddings = torch.stack(batch_vid_embeddings)\n",
    "    \n",
    "    # MAKE SURE VECTORS ARE NORMALIZED FIRST idk if I want to do here or later..\n",
    "    batch_aud_embeddings = torch.nn.functional.normalize(batch_aud_embeddings, p=2, dim=1)\n",
    "    batch_vid_embeddings = torch.nn.functional.normalize(batch_vid_embeddings, p=2, dim=1)\n",
    "\n",
    "    return batch_aud_embeddings, batch_vid_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "8b4e9853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qb/2jqy46j9757g7m30b7cz_z040000gn/T/ipykernel_42012/3547236906.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vf = torch.tensor(video_features[start_segment:end_segment,:])\n",
      "/var/folders/qb/2jqy46j9757g7m30b7cz_z040000gn/T/ipykernel_42012/3547236906.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  af = torch.tensor(audio_features[start_segment:end_segment,:])\n"
     ]
    }
   ],
   "source": [
    "batch_aud_embeddings, batch_vid_embeddings = get_batch_embeddings(video_model, audio_model, video_batch, audio_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a7cdd750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE VECTORS ARE NORMALIZED FIRST\n",
    "# embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "video_similities = torch.matmul(batch_vid_embeddings, batch_vid_embeddings.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "88a574e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k(batch_embeddings):\n",
    "    similarity = torch.matmul(batch_embeddings, batch_embeddings.T)\n",
    "\n",
    "    # Set diagonal to -inf to exclude self-similarity (or you could set it to NaN)\n",
    "    # This is done so the \"most similar\" excludes the embedding itself.\n",
    "    similarity.fill_diagonal_(-float('inf'))\n",
    "\n",
    "    # Find the top k most similar embeddings for each embedding\n",
    "    k = 5  # Number of similar embeddings to find\n",
    "    top_k_similarities, top_k_indices = torch.topk(similarity, k, dim=1, largest=True)\n",
    "\n",
    "    # Print the results\n",
    "    #print(\"Top k most similar embeddings:\")\n",
    "    #for i in range(batch_embeddings.shape[0]):\n",
    "    #    print(f\"Embedding {i} - Most Similar Embeddings (Indices): {top_k_indices[i]}\")\n",
    "    #    print(f\"Cosine Similarities: {top_k_similarities[i]}\")\n",
    "    return top_k_similarities, top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e4494c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1292, -0.9560, -1.0216, -1.2049, -0.9785, -0.9997, -1.2286, -0.7535,\n",
       "        -0.8464, -0.9180, -0.7393, -0.9416, -0.9702, -1.0371, -0.7102, -0.8329,\n",
       "        -1.1699, -1.0593, -0.9823, -0.8386, -1.4440, -0.9205],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.normalize(batch_vid_embeddings, p=2, dim=1).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "d9f5d801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6,  3,  2, 20,  8])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_similarities, top_k_indices = get_top_k(batch_vid_embeddings)\n",
    "top_k_indices[0]\n",
    "# so for vid 0, we will use negative anchors 3, 4, 7, 5 and 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b6477a",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "b2b7991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b69481f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda1 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "39c06cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda2 = 0.1\n",
    "lambda3 = 0.1\n",
    "lambda4 = 0.1\n",
    "lambda5 = 0.1\n",
    "lambda6 = 0.1\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "# Define the Adam optimizer for the audio model\n",
    "audio_optimizer = optim.Adam(audio_model.parameters(), lr=lr)\n",
    "\n",
    "# Define the Adam optimizer for the video model\n",
    "video_optimizer = optim.Adam(video_model.parameters(), lr=lr)\n",
    "\n",
    "batch_size = 2\n",
    "window_size = 20 # for optical flow smoothing (ie 20 frame mavg flow)\n",
    "\n",
    "dataloader = get_dataloader(path, batch_size=batch_size, shuffle=True, method='video', window_size=window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b97b0",
   "metadata": {},
   "source": [
    "### Showing how to mine negative embeddings in vectorized way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "88bec5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qb/2jqy46j9757g7m30b7cz_z040000gn/T/ipykernel_42012/3547236906.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vf = torch.tensor(video_features[start_segment:end_segment,:])\n",
      "/var/folders/qb/2jqy46j9757g7m30b7cz_z040000gn/T/ipykernel_42012/3547236906.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  af = torch.tensor(audio_features[start_segment:end_segment,:])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  6,  2, 20,  8],\n",
       "        [ 4,  9,  5,  8, 11],\n",
       "        [ 3,  0,  6,  8,  5],\n",
       "        [ 0,  6,  2, 20, 13],\n",
       "        [ 9,  8,  5,  1, 11],\n",
       "        [ 4,  8,  9,  1, 11],\n",
       "        [ 0,  3,  2, 20, 13],\n",
       "        [ 1, 11,  8,  4,  9],\n",
       "        [ 9,  4,  5,  1, 11],\n",
       "        [ 8,  4,  5,  1, 11],\n",
       "        [ 7, 11,  1,  4,  9],\n",
       "        [21,  1,  9,  4,  8],\n",
       "        [21, 11, 19, 13,  9],\n",
       "        [17, 18,  8, 12, 20],\n",
       "        [19, 15, 17, 13,  9],\n",
       "        [14, 21, 19, 11,  9],\n",
       "        [ 3,  6,  0,  2, 13],\n",
       "        [18, 13, 19,  8,  9],\n",
       "        [17, 13, 19,  8,  9],\n",
       "        [17, 18, 14, 21, 12],\n",
       "        [13,  2, 18, 17,  3],\n",
       "        [12, 11, 15, 19,  9]])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_aud_embeddings, batch_vid_embeddings = get_batch_embeddings(video_model, audio_model, video_batch, audio_batch)\n",
    "\n",
    "_, vid_top_k = get_top_k(batch_vid_embeddings)\n",
    "vid_top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "2d5d2039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 5, 256])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_vid_embeddings[vid_top_k].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "2f67c9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 256])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_vid_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "11aaa091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 1, 256])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_vid_embeddings.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "15404d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 5, 256])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((batch_vid_embeddings.unsqueeze(1))*batch_vid_embeddings[vid_top_k]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1122f3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9974, 0.9972, 0.9937, 0.9794, 0.9752], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((batch_vid_embeddings.unsqueeze(1))*batch_vid_embeddings[vid_top_k])[0].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "0eb2dac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9550, 0.9706, 0.9937, 0.9725, 0.9752], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([batch_vid_embeddings[0]*batch_vid_embeddings[7],\n",
    "             batch_vid_embeddings[0]*batch_vid_embeddings[4],\n",
    "             batch_vid_embeddings[0]*batch_vid_embeddings[2],\n",
    "             batch_vid_embeddings[0]*batch_vid_embeddings[5],\n",
    "             batch_vid_embeddings[0]*batch_vid_embeddings[8]]).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "665d6e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9550, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(batch_vid_embeddings[0],batch_vid_embeddings[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "3a8e30e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9706, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(batch_vid_embeddings[0],batch_vid_embeddings[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bbf64b",
   "metadata": {},
   "source": [
    "### Showing how to get positive embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "2b723384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0242, 0.0371, 0.0467, 0.0355, 0.0358, 0.0414, 0.0277, 0.0235, 0.0361,\n",
       "        0.0443, 0.0335, 0.0372, 0.0560, 0.0570, 0.0380, 0.0585, 0.0453, 0.0495,\n",
       "        0.0545, 0.0459, 0.0617, 0.0424], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(batch_vid_embeddings*batch_aud_embeddings).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "54a42852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.3245e-03,  4.1721e-03,  2.2809e-03,  ...,  4.1571e-03,\n",
       "          2.5281e-03, -1.6599e-03],\n",
       "        [ 3.9830e-03,  6.2003e-03,  2.3320e-03,  ...,  5.2013e-03,\n",
       "          8.6622e-04, -5.2390e-04],\n",
       "        [ 2.6704e-03,  4.9460e-03,  2.8417e-03,  ...,  4.4937e-03,\n",
       "          2.8771e-03, -1.6045e-03],\n",
       "        ...,\n",
       "        [ 4.1135e-03,  1.1327e-02,  2.9570e-03,  ...,  6.3373e-03,\n",
       "          8.8758e-04, -2.2910e-04],\n",
       "        [ 3.5647e-03,  7.9073e-03,  2.0349e-03,  ...,  4.6994e-03,\n",
       "          1.5677e-03,  2.3363e-04],\n",
       "        [ 4.0173e-03,  1.0205e-02,  1.8800e-03,  ...,  6.0003e-03,\n",
       "          5.6711e-04,  3.1909e-05]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### total loss vectorized\n",
    "(batch_vid_embeddings*batch_aud_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "8897a372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9974, 0.9972, 0.9937, 0.9794, 0.9752],\n",
       "        [0.9963, 0.9954, 0.9948, 0.9937, 0.9919],\n",
       "        [0.9944, 0.9937, 0.9918, 0.9875, 0.9873],\n",
       "        [0.9974, 0.9967, 0.9944, 0.9818, 0.9788],\n",
       "        [0.9970, 0.9970, 0.9965, 0.9963, 0.9908],\n",
       "        [0.9965, 0.9961, 0.9961, 0.9948, 0.9880],\n",
       "        [0.9972, 0.9967, 0.9918, 0.9773, 0.9733],\n",
       "        [0.9901, 0.9886, 0.9879, 0.9876, 0.9874],\n",
       "        [0.9974, 0.9970, 0.9961, 0.9937, 0.9899],\n",
       "        [0.9974, 0.9970, 0.9961, 0.9954, 0.9916],\n",
       "        [0.9841, 0.9748, 0.9734, 0.9729, 0.9727],\n",
       "        [0.9940, 0.9919, 0.9916, 0.9908, 0.9899],\n",
       "        [0.9964, 0.9896, 0.9887, 0.9870, 0.9866],\n",
       "        [0.9944, 0.9923, 0.9872, 0.9870, 0.9868],\n",
       "        [0.9894, 0.9891, 0.9870, 0.9866, 0.9854],\n",
       "        [0.9891, 0.9890, 0.9885, 0.9873, 0.9856],\n",
       "        [0.9697, 0.9695, 0.9677, 0.9568, 0.9559],\n",
       "        [0.9952, 0.9944, 0.9905, 0.9885, 0.9875],\n",
       "        [0.9952, 0.9923, 0.9897, 0.9869, 0.9858],\n",
       "        [0.9905, 0.9897, 0.9894, 0.9889, 0.9887],\n",
       "        [0.9868, 0.9857, 0.9825, 0.9820, 0.9818],\n",
       "        [0.9964, 0.9940, 0.9890, 0.9889, 0.9873]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_sims = (((batch_vid_embeddings.unsqueeze(1))*batch_vid_embeddings[vid_top_k])).sum(axis=-1)\n",
    "top_k_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "35c37e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0242],\n",
       "        [0.0371],\n",
       "        [0.0467],\n",
       "        [0.0355],\n",
       "        [0.0358],\n",
       "        [0.0414],\n",
       "        [0.0277],\n",
       "        [0.0235],\n",
       "        [0.0361],\n",
       "        [0.0443],\n",
       "        [0.0335],\n",
       "        [0.0372],\n",
       "        [0.0560],\n",
       "        [0.0570],\n",
       "        [0.0380],\n",
       "        [0.0585],\n",
       "        [0.0453],\n",
       "        [0.0495],\n",
       "        [0.0545],\n",
       "        [0.0459],\n",
       "        [0.0617],\n",
       "        [0.0424]], grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_sims = ((batch_vid_embeddings*batch_aud_embeddings).sum(axis=1)).unsqueeze(1)\n",
    "pos_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "caf8ed84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]], grad_fn=<ClampBackward1>)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clip(pos_sims - top_k_sims,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "34d8b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.sum(torch.clip(pos_sims - top_k_sims,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "48de3380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qb/2jqy46j9757g7m30b7cz_z040000gn/T/ipykernel_42012/3547236906.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vf = torch.tensor(video_features[start_segment:end_segment,:])\n",
      "/var/folders/qb/2jqy46j9757g7m30b7cz_z040000gn/T/ipykernel_42012/3547236906.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  af = torch.tensor(audio_features[start_segment:end_segment,:])\n"
     ]
    }
   ],
   "source": [
    "# Batch iterator\n",
    "for video_batch, audio_batch, flow_ranks in dataloader:\n",
    "    audio_optimizer.zero_grad()\n",
    "    video_optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "\n",
    "    # create segments for each batch and compute embeddings for the segments\n",
    "    # stack all the embeddings into single tensors\n",
    "    batch_aud_embeddings, batch_vid_embeddings = get_batch_embeddings(video_model, audio_model, video_batch, audio_batch)\n",
    "\n",
    "    # for each video embedding find the k most similar video embeddings\n",
    "    _, vid_top_k = get_top_k(batch_vid_embeddings)\n",
    "    \n",
    "    # for each audio embedding, find the k most simlar audio embeddings\n",
    "    _, aud_top_k = get_top_k(batch_aud_embeddings)\n",
    "\n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2a28cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
