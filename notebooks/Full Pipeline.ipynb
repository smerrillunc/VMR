{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "b15aa730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from scipy.signal import peak_prominences\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import itertools\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623fb853",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"/Users/scottmerrill/Documents/UNC/MultiModal/VMR/Youtube8m\"\n",
    "filenames = os.listdir(path + '/video')\n",
    "file_name = filenames[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02ecae",
   "metadata": {},
   "source": [
    "### 1.  DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a91e4388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoAudioDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.filenames = os.listdir(os.path.join(path, 'video'))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        video_data = np.load(os.path.join(self.path, 'video', filename))\n",
    "        audio_data = np.load(os.path.join(self.path, 'audio', filename))\n",
    "        video_data = video_data[:, :1024]\n",
    "        return video_data, audio_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60269016",
   "metadata": {},
   "source": [
    "### 2. Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8455610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim=1024, embed_dim=512, num_heads=8, num_layers=2, max_seq_len=50):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, embed_dim)  # Project input to embedding dim\n",
    "        self.pos_encoder = self._generate_sinusoidal_positional_encoding(max_seq_len, embed_dim)\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads), num_layers=num_layers)\n",
    "        self.output_proj = nn.Linear(embed_dim, embed_dim)  # Project to final embedding\n",
    "\n",
    "    def _generate_sinusoidal_positional_encoding(self, max_len, embed_dim):\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(math.log(10000.0) / embed_dim))\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # Shape: (1, max_len, embed_dim)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_proj(x)  # Shape: (seq_len, embed_dim)\n",
    "        seq_len = x.size(0)\n",
    "        x = x + self.pos_encoder[:, :seq_len, :].squeeze(0).to(x.device)\n",
    "        x = self.transformer(x.unsqueeze(1), src_key_padding_mask=mask).squeeze(1)\n",
    "        x = x.mean(dim=0)  # Aggregate sequence to fixed-size embedding\n",
    "        return self.output_proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13aa8cc",
   "metadata": {},
   "source": [
    "### Optical Flow Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "dabc43ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpticalFlowProcessor:\n",
    "    def __init__(self, method='video', window_size=20, max_segments=10, min_frames=10):\n",
    "        self.method = method\n",
    "        self.window_size = window_size\n",
    "        self.max_segments = max_segments\n",
    "        self.min_frames = min_frames\n",
    "\n",
    "    def get_of_ranks(self, rgb, audio):\n",
    "        flow = self._compute_flow(rgb, audio)\n",
    "        segments = self._optical_flow_segments(flow)\n",
    "        ranks = self._rank_averages(self._compute_segment_means(segments, flow))\n",
    "        return ranks\n",
    "\n",
    "    def get_best_worst_flow(self, rgb, audio):\n",
    "        flow = self._compute_flow(rgb, audio)\n",
    "        segments = self._optical_flow_segments(flow)\n",
    "        ranks = self._rank_averages(self._compute_segment_means(segments, flow))\n",
    "        return self._extract_best_worst_segments(segments, ranks)\n",
    "\n",
    "    def _compute_flow(self, rgb, audio):\n",
    "        if self.method == 'video':\n",
    "            return self._moving_average(self._calculate_optical_flow_euclidean(rgb))\n",
    "        elif self.method == 'audio':\n",
    "            return self._moving_average(self._calculate_optical_flow_euclidean(audio))\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'video' or 'audio'\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_optical_flow_euclidean(embedding_seq):\n",
    "        return np.linalg.norm(embedding_seq[1:] - embedding_seq[:-1], axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _moving_average(arr, window_size=5):\n",
    "        return np.convolve(arr, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "    def _optical_flow_segments(self, optical_flow):\n",
    "        peaks, _ = find_peaks(optical_flow)\n",
    "        prominences = peak_prominences(optical_flow, peaks)[0]\n",
    "        peak_index = peaks[np.argsort(prominences)[-self.max_segments:]]\n",
    "        peak_index = self._merge_intervals(np.sort(peak_index))\n",
    "        return np.insert(np.append(peak_index, len(optical_flow)), 0, 0)\n",
    "\n",
    "    def _merge_intervals(self, arr):\n",
    "        merged = [arr[0]]\n",
    "        for i in range(1, len(arr)):\n",
    "            if arr[i] - merged[-1] >= self.min_frames:\n",
    "                merged.append(arr[i])\n",
    "        return np.array(merged)\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_segment_means(segments, values):\n",
    "        return [values[start:end].mean() if start < end else 0 for start, end in zip(segments[:-1], segments[1:])]\n",
    "\n",
    "    @staticmethod\n",
    "    def _rank_averages(averages):\n",
    "        sorted_indices = np.argsort(averages)[::-1]\n",
    "        ranks = np.zeros_like(sorted_indices) + 1\n",
    "        for rank, idx in enumerate(sorted_indices):\n",
    "            ranks[idx] = rank + 1\n",
    "        return ranks\n",
    "\n",
    "    def _extract_best_worst_segments(self, segments, ranks):\n",
    "        top_start, top_end = segments[np.where(ranks == 1)[0][0]], segments[np.where(ranks == 1)[0][0] + 1]\n",
    "        bottom_start, bottom_end = segments[np.where(ranks == max(ranks))[0][0]], segments[np.where(ranks == max(ranks))[0][0] + 1]\n",
    "        return (top_start, top_end), (bottom_start, bottom_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c312d3d",
   "metadata": {},
   "source": [
    "### Script Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ee01b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, processor):\n",
    "    video_batch, audio_batch = zip(*batch)\n",
    "    video_batch = [torch.tensor(v, dtype=torch.float32) for v in video_batch]\n",
    "    audio_batch = [torch.tensor(a, dtype=torch.float32) for a in audio_batch]\n",
    "    flow_ranks = [processor.get_of_ranks(video_batch[i], audio_batch[i]) for i in range(len(video_batch))]\n",
    "    return video_batch, audio_batch, flow_ranks\n",
    "\n",
    "def get_dataloader(path, batch_size=32, shuffle=True, method='video', window_size=20):\n",
    "    dataset = VideoAudioDataset(path)\n",
    "    processor = OpticalFlowProcessor(method=method, window_size=window_size)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=lambda batch: collate_fn(batch, processor))\n",
    "\n",
    "def perform_feature_padding(video_features, audio_features, start_segment, end_segment, max_seq_len):\n",
    "    vf = torch.tensor(video_features[start_segment:end_segment,:])\n",
    "    af = torch.tensor(audio_features[start_segment:end_segment,:])\n",
    "\n",
    "    pvf = torch.zeros(max_seq_len, 1024)\n",
    "    pvf[:vf.shape[0], :] = vf\n",
    "\n",
    "    paf = torch.zeros(max_seq_len, 128)\n",
    "    paf[:af.shape[0], :] = af\n",
    "\n",
    "    # Create mask (True for padding positions)\n",
    "    mask = torch.arange(max_seq_len) >= vf.shape[0]\n",
    "    mask = mask.unsqueeze(0)  # Convert to 2D (batch_size=1, seq_len)\n",
    "    return pvf, paf, mask\n",
    "\n",
    "# Function to find pairs with approximately equal differences\n",
    "def find_matching_index_pairs(array1, array2, tolerance=5):\n",
    "    # Calculate differences in array1 and array2\n",
    "    array1_diffs = np.diff(array1)\n",
    "    array2_diffs = np.diff(array2)\n",
    "\n",
    "    matching_pairs = []\n",
    "\n",
    "    # Loop through differences in array1\n",
    "    for i, diff1 in enumerate(array1_diffs):\n",
    "        # Find pairs of consecutive indices in array2 with similar differences\n",
    "        for j, diff2 in enumerate(array2_diffs):\n",
    "            if abs(diff1 - diff2) <= tolerance:  # If the difference is within the tolerance\n",
    "                matching_pairs.append(((i, i + 1), (j, j + 1), diff1, diff2))\n",
    "\n",
    "    return matching_pairs\n",
    "\n",
    "\n",
    "def get_similar_length_segments(positive_segments, negative_segments, tolerance = 5):\n",
    "    \n",
    "    while True:\n",
    "        matching_indexes = find_matching_index_pairs(positive_segments, negative_segments, tolerance=tolerance)\n",
    "        tolerance += 5\n",
    "        if len(matching_indexes) > 0:\n",
    "            break\n",
    "            \n",
    "    # sample randomly for all segments within the tolerance band\n",
    "    pos_segment, negative_segment, pos_time, neg_time = matching_indexes[np.random.randint(0, len(matching_indexes))]\n",
    "    \n",
    "    return pos_segment, negative_segment, pos_time, neg_time\n",
    "\n",
    "def get_positive_negative_embeddings(filenames, tolerance=5):\n",
    "\n",
    "    positive_record_file = np.random.choice(filenames)\n",
    "    negative_record_file = np.random.choice(filenames)\n",
    "\n",
    "    positive_rgb, positive_audio = sample_dataset(positive_record_file)\n",
    "    negative_rgb, negative_audio = sample_dataset(negative_record_file)\n",
    "\n",
    "    optical_flow_pos = calculate_optical_flow_euclidean(positive_rgb)\n",
    "    optical_flow_pos = moving_average(optical_flow_pos, window_size=20)\n",
    "\n",
    "    optical_flow_neg = calculate_optical_flow_euclidean(negative_rgb)\n",
    "    optical_flow_neg = moving_average(optical_flow_neg, window_size=20)\n",
    "\n",
    "    positive_segments = optical_flow_segments(optical_flow_pos)\n",
    "    negative_segments = optical_flow_segments(optical_flow_neg)\n",
    "\n",
    "    pos_segment, negative_segment, pos_time, neg_time = get_similar_length_segments(positive_segments, negative_segments, tolerance = 5)\n",
    "\n",
    "    # converting segment index to time in seconds\n",
    "    pos_start, pos_end = pos_segment\n",
    "    pos_start = positive_segments[pos_start]\n",
    "    pos_end = positive_segments[pos_end]\n",
    "    print(pos_start, pos_end)\n",
    "\n",
    "    # retrieving segment and padding it appropriately\n",
    "    pos_video, pos_audio, pos_mask = perform_feature_padding(positive_rgb, positive_audio, pos_start, pos_end, max_seq_len)\n",
    "\n",
    "    # computing embedding\n",
    "    positive_video_embedding = video_model(pos_video, pos_mask)\n",
    "    positive_audio_embedding = audio_model(pos_audio, pos_mask)\n",
    "\n",
    "    # converting segment index to time in seconds\n",
    "    neg_start, neg_end = negative_segment\n",
    "    neg_start = negative_segments[neg_start]\n",
    "    neg_end = negative_segments[neg_end]\n",
    "\n",
    "    # retrieving segment and padding it appropriately\n",
    "    neg_video, neg_audio, neg_mask = perform_feature_padding(negative_rgb, negative_audio, neg_start, neg_end, max_seq_len)\n",
    "\n",
    "    # computing embedding\n",
    "    negative_video_embedding = video_model(neg_video, neg_mask)\n",
    "    negative_audio_embedding = audio_model(neg_audio, neg_mask)\n",
    "    return positive_video_embedding, positive_audio_embedding, negative_video_embedding, negative_audio_embedding\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, filename):\n",
    "    \"\"\"Saves model and optimizer state dict.\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved at epoch {epoch} to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "8e511c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positive_negative_embeddings(video_batch, tolerance=5):\n",
    "\n",
    "    positive_record_file = np.random.choice(filenames)\n",
    "    negative_record_file = np.random.choice(filenames)\n",
    "\n",
    "    positive_rgb, positive_audio = sample_dataset(positive_record_file)\n",
    "    negative_rgb, negative_audio = sample_dataset(negative_record_file)\n",
    "\n",
    "    optical_flow_pos = calculate_optical_flow_euclidean(positive_rgb)\n",
    "    optical_flow_pos = moving_average(optical_flow_pos, window_size=20)\n",
    "\n",
    "    optical_flow_neg = calculate_optical_flow_euclidean(negative_rgb)\n",
    "    optical_flow_neg = moving_average(optical_flow_neg, window_size=20)\n",
    "\n",
    "    positive_segments = optical_flow_segments(optical_flow_pos)\n",
    "    negative_segments = optical_flow_segments(optical_flow_neg)\n",
    "\n",
    "    pos_segment, negative_segment, pos_time, neg_time = get_similar_length_segments(positive_segments, negative_segments, tolerance = 5)\n",
    "\n",
    "    # converting segment index to time in seconds\n",
    "    pos_start, pos_end = pos_segment\n",
    "    pos_start = positive_segments[pos_start]\n",
    "    pos_end = positive_segments[pos_end]\n",
    "    print(pos_start, pos_end)\n",
    "\n",
    "    # retrieving segment and padding it appropriately\n",
    "    pos_video, pos_audio, pos_mask = perform_feature_padding(positive_rgb, positive_audio, pos_start, pos_end, max_seq_len)\n",
    "\n",
    "    # computing embedding\n",
    "    positive_video_embedding = video_model(pos_video, pos_mask)\n",
    "    positive_audio_embedding = audio_model(pos_audio, pos_mask)\n",
    "\n",
    "    # converting segment index to time in seconds\n",
    "    neg_start, neg_end = negative_segment\n",
    "    neg_start = negative_segments[neg_start]\n",
    "    neg_end = negative_segments[neg_end]\n",
    "\n",
    "    # retrieving segment and padding it appropriately\n",
    "    neg_video, neg_audio, neg_mask = perform_feature_padding(negative_rgb, negative_audio, neg_start, neg_end, max_seq_len)\n",
    "\n",
    "    # computing embedding\n",
    "    negative_video_embedding = video_model(neg_video, neg_mask)\n",
    "    negative_audio_embedding = audio_model(neg_audio, neg_mask)\n",
    "    return positive_video_embedding, positive_audio_embedding, negative_video_embedding, negative_audio_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "af74f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 100\n",
    "\n",
    "\n",
    "def get_segmentd_embeddings(video_model, audio_model, vid, aud):\n",
    "    vid_segment_embeddings = []\n",
    "    \n",
    "\n",
    "    of = OpticalFlowProcessor()\n",
    "    flow = of._compute_flow(vid, aud)\n",
    "    segments = of._optical_flow_segments(flow)\n",
    "\n",
    "    vid_segment_embeddings = []\n",
    "    aud_segment_embeddings = []\n",
    "    for i in range(1, len(segments)):\n",
    "        start = segments[i-1]\n",
    "        end = segments[i]\n",
    "\n",
    "        vid_emb, aud_emb, mask = perform_feature_padding(vid, aud, start, end, max_seq_len)\n",
    "        \n",
    "        vid_segment_embeddings.append(video_model(vid_emb, mask))\n",
    "        aud_segment_embeddings.append(audio_model(aud_emb, mask))\n",
    "    return vid_segment_embeddings, aud_segment_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6ac618a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads =1\n",
    "num_layers=1\n",
    "audio_model = Transformer(input_dim=128, embed_dim=256, num_heads=num_heads, num_layers=num_layers, max_seq_len=max_seq_len)\n",
    "video_model = Transformer(input_dim=1024, embed_dim=256, num_heads=num_heads, num_layers=num_layers, max_seq_len=max_seq_len)\n",
    "\n",
    "def get_batch_embeddings(video_model, audio_model, video_batch, audio_batch):\n",
    "    # We precompute the segment embeddings in each batch.  We do this once and then proceed to processing batch\n",
    "    batch_vid_embeddings = []\n",
    "    batch_aud_embeddings = []\n",
    "    for i in range(len(video_batch)):\n",
    "        vid = video_batch[i]\n",
    "        aud = audio_batch[i]\n",
    "        vid_sgmt_emb, aud_sgmt_emb = get_segmentd_embeddings(video_model, audio_model, vid, aud)\n",
    "        batch_vid_embeddings.extend(vid_sgmt_emb)\n",
    "        batch_aud_embeddings.extend(aud_sgmt_emb)\n",
    "        \n",
    "    # Shape will by (total segments X embedding dim)\n",
    "    # total segments is clip dependent\n",
    "    batch_aud_embeddings = torch.stack(batch_aud_embeddings)\n",
    "    batch_vid_embeddings = torch.stack(batch_vid_embeddings)\n",
    "    \n",
    "    # MAKE SURE VECTORS ARE NORMALIZED FIRST idk if I want to do here or later..\n",
    "    batch_aud_embeddings = torch.nn.functional.normalize(batch_aud_embeddings, p=2, dim=1)\n",
    "    batch_vid_embeddings = torch.nn.functional.normalize(batch_vid_embeddings, p=2, dim=1)\n",
    "\n",
    "    return batch_aud_embeddings, batch_vid_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e4494c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intermodal_loss(batch_vid_embeddings, batch_aud_embeddings, k=5, min_val=0):\n",
    "    # batch_vid_embeddings and  batch_aud_embeddings should already be normalized so \n",
    "    # multiplying them is a similarity metric\n",
    "    \n",
    "    # convert simliarity to distance by (-1) >> high value indicates the distance between the samples is long\n",
    "    dist_xy = (-1) *torch.matmul(batch_vid_embeddings, batch_aud_embeddings.T)\n",
    "    \n",
    "    positive_pairs = torch.diag(dist_xy)\n",
    "\n",
    "    # Get non-diagonal elements (negative examples)\n",
    "    # First, create a mask for non-diagonal elements\n",
    "    mask = ~torch.eye(dist_xy.size(0), dtype=torch.bool)\n",
    "\n",
    "    # Apply the mask to extract non-diagonal elements\n",
    "    negative_pairs = dist_xy[mask]\n",
    "    \n",
    "    topk_pos_values, _ = torch.topk(positive_pairs.flatten(), k)\n",
    "    topk_neg_values, _ = torch.topk(negative_pairs.flatten(), k)\n",
    "    \n",
    "    # max accross each pos/neg pair\n",
    "    loss = torch.max(topk_pos_values - topk_neg_values, torch.tensor(min_val))\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f5d801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95b6477a",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b2b7991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b69481f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "39c06cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda1 = 0.33\n",
    "lambda2 = 0.33\n",
    "lambda3 = 0.33\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "# Define the Adam optimizer for the audio model\n",
    "audio_optimizer = optim.Adam(audio_model.parameters(), lr=lr)\n",
    "\n",
    "# Define the Adam optimizer for the video model\n",
    "video_optimizer = optim.Adam(video_model.parameters(), lr=lr)\n",
    "\n",
    "batch_size = 2\n",
    "window_size = 20 # for optical flow smoothing (ie 20 frame mavg flow)\n",
    "\n",
    "dataloader = get_dataloader(path, batch_size=batch_size, shuffle=True, method='video', window_size=window_size)\n",
    "num_flow_matching = 10\n",
    "k = 20\n",
    "margin = 0.1\n",
    "triplet_loss = nn.TripletMarginLoss(margin=margin, p=4, eps=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "48de3380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qb/2jqy46j9757g7m30b7cz_z040000gn/T/ipykernel_60234/3764631200.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vf = torch.tensor(video_features[start_segment:end_segment,:])\n",
      "/var/folders/qb/2jqy46j9757g7m30b7cz_z040000gn/T/ipykernel_60234/3764631200.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  af = torch.tensor(audio_features[start_segment:end_segment,:])\n"
     ]
    }
   ],
   "source": [
    "# Batch iterator\n",
    "for video_batch, audio_batch, flow_ranks in dataloader:\n",
    "    try:\n",
    "        audio_optimizer.zero_grad()\n",
    "        video_optimizer.zero_grad()\n",
    "        total_loss = 0\n",
    "\n",
    "        # create segments for each batch and compute embeddings for the segments\n",
    "        # stack all the embeddings into single tensors\n",
    "        batch_aud_embeddings, batch_vid_embeddings = get_batch_embeddings(video_model, audio_model, video_batch, audio_batch)\n",
    "\n",
    "        \n",
    "        # 1. Inter-modal loss\n",
    "        inter_modal_loss = get_intermodal_loss(batch_vid_embeddings, batch_aud_embeddings, k=5, min_val=0)\n",
    "\n",
    "\n",
    "        # 2. optical flow loss\n",
    "\n",
    "        # this code finds the top and bottom ranked optical flow for a particular\n",
    "        # video.  This is specified in flow ranks.  It then converts these indexes to \n",
    "        # their corresponding position in the stacked embeddings\n",
    "        top_rank_idxs = []\n",
    "        bottom_rank_idxs = []\n",
    "        current_index = 0\n",
    "        for ranks in flow_ranks:\n",
    "            top_rank_idxs.append(current_index + np.argmin(ranks))\n",
    "            bottom_rank_idxs.append(current_index + np.argmax(ranks))\n",
    "            current_index += len(ranks)\n",
    "\n",
    "        # Randomly choose num_flow_matching pairs to match (top_rank, top_rank, bottom rank)\n",
    "        top_matching_samples = list(itertools.product(top_rank_idxs, top_rank_idxs, bottom_rank_idxs))\n",
    "        top_matching_samples = [random.choice(top_matching_samples) for _ in range(num_flow_matching)]\n",
    "\n",
    "        # Randomly choose num_flow_matching pairs to match (bottom, bottom, top_rank rank)\n",
    "        bottom_matching_samples = list(itertools.product(bottom_rank_idxs, bottom_rank_idxs, top_rank_idxs))\n",
    "        bottom_matching_samples = [random.choice(bottom_matching_samples) for _ in range(num_flow_matching)]\n",
    "\n",
    "        of_loss_top = 0\n",
    "        for anchor, pos, neg in top_matching_samples:\n",
    "            of_loss_top += triplet_loss(batch_vid_embeddings[anchor], batch_vid_embeddings[pos], batch_vid_embeddings[neg])\n",
    "            of_loss_top += triplet_loss(batch_aud_embeddings[anchor], batch_aud_embeddings[pos], batch_aud_embeddings[neg])\n",
    "\n",
    "        of_loss_bottom = 0\n",
    "        for anchor, pos, neg in bottom_matching_samples:\n",
    "            of_loss_bottom += triplet_loss(batch_vid_embeddings[anchor], batch_vid_embeddings[pos], batch_vid_embeddings[neg])\n",
    "            of_loss_bottom += triplet_loss(batch_aud_embeddings[anchor], batch_aud_embeddings[pos], batch_aud_embeddings[neg])\n",
    "\n",
    "        loss = lambda1*inter_modal_loss + lambda2*of_loss_top + lambda3*of_loss_bottom\n",
    "        loss.backward()\n",
    "        audio_optimizer.step()\n",
    "        video_optimizer.step()\n",
    "        \n",
    "    except Exception as e:\n",
    "        # adding a wrapper just in case\n",
    "        print(e)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "9c03fa34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1173, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab13eee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
