{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ccea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from scipy.signal import peak_prominences\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import itertools\n",
    "import random\n",
    "import ruptures as rpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623fb853",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"/Users/scottmerrill/Documents/UNC/MultiModal/VMR/Youtube8m\"\n",
    "filenames = os.listdir(path + '/video')\n",
    "file_name = filenames[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02ecae",
   "metadata": {},
   "source": [
    "### 1.  DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a91e4388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoAudioDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.filenames = os.listdir(os.path.join(path, 'video'))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        video_data = np.load(os.path.join(self.path, 'video', filename))\n",
    "        audio_data = np.load(os.path.join(self.path, 'audio', filename))\n",
    "        video_data = video_data[:, :1024]\n",
    "        return video_data, audio_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60269016",
   "metadata": {},
   "source": [
    "### 2. Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b8455610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim=1024, embed_dim=512, num_heads=8, num_layers=2, max_seq_len=50):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, embed_dim)  # Project input to embedding dim\n",
    "        self.pos_encoder = self._generate_sinusoidal_positional_encoding(max_seq_len, embed_dim)\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads), num_layers=num_layers)\n",
    "        self.output_proj = nn.Linear(embed_dim, embed_dim)  # Project to final embedding\n",
    "\n",
    "    def _generate_sinusoidal_positional_encoding(self, max_len, embed_dim):\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(math.log(10000.0) / embed_dim))\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # Shape: (1, max_len, embed_dim)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_proj(x)  # Shape: (seq_len, embed_dim)\n",
    "        seq_len = x.size(0)\n",
    "        x = x + self.pos_encoder[:, :seq_len, :].squeeze(0).to(x.device)\n",
    "        x = self.transformer(x.unsqueeze(1), src_key_padding_mask=mask).squeeze(1)\n",
    "        x = x.mean(dim=0)  # Aggregate sequence to fixed-size embedding\n",
    "        return self.output_proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13aa8cc",
   "metadata": {},
   "source": [
    "### Optical Flow Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "dabc43ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpticalFlowProcessor:\n",
    "    def __init__(self, method='video', window_size=10, min_segments=9, min_frames=10):\n",
    "        self.method = method\n",
    "        self.window_size = window_size\n",
    "        self.min_segments = min_segments\n",
    "        self.min_frames = min_frames\n",
    "\n",
    "    def get_of_ranks(self, rgb, audio):\n",
    "        flow = self._compute_flow(rgb, audio)\n",
    "        segments = self._optical_flow_segments(flow)\n",
    "        ranks = self._rank_averages(self._compute_segment_means(segments, flow))\n",
    "        return ranks\n",
    "\n",
    "    def get_best_worst_flow(self, rgb, audio):\n",
    "        flow = self._compute_flow(rgb, audio)\n",
    "        segments = self._optical_flow_segments(flow)\n",
    "        ranks = self._rank_averages(self._compute_segment_means(segments, flow))\n",
    "        return self._extract_best_worst_segments(segments, ranks)\n",
    "\n",
    "    def _compute_flow(self, rgb, audio):\n",
    "        if self.method == 'video':\n",
    "            return self._moving_average(self._calculate_optical_flow_euclidean(rgb))\n",
    "        elif self.method == 'audio':\n",
    "            return self._moving_average(self._calculate_optical_flow_euclidean(audio))\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'video' or 'audio'\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_optical_flow_euclidean(embedding_seq):\n",
    "        return np.linalg.norm(embedding_seq[1:] - embedding_seq[:-1], axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _moving_average(arr, window_size=5):\n",
    "        return np.convolve(arr, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "    def _optical_flow_segments_old(self, optical_flow):\n",
    "        peaks, _ = find_peaks(optical_flow)\n",
    "        prominences = peak_prominences(optical_flow, peaks)[0]\n",
    "        peak_index = peaks[np.argsort(prominences)[-self.max_segments:]]\n",
    "        peak_index = self._merge_intervals(np.sort(peak_index))\n",
    "        return np.insert(np.append(peak_index, len(optical_flow)), 0, 0)\n",
    "\n",
    "    def _optical_flow_segments(self, optical_flow_video, max_seq_len=100):\n",
    "    \n",
    "        algo = rpt.Dynp(model='l2', min_size=self.min_segments, jump=3).fit(optical_flow_video)\n",
    "        change_points = algo.predict(n_bkps=min_segments)  # The 'pen' parameter controls sensitivity\n",
    "\n",
    "        # insert zero for start segment\n",
    "        change_points.insert(0,0)\n",
    "        return change_points\n",
    "\n",
    "    \n",
    "    def _merge_intervals(self, arr):\n",
    "        merged = [arr[0]]\n",
    "        for i in range(1, len(arr)):\n",
    "            if arr[i] - merged[-1] >= self.min_frames:\n",
    "                merged.append(arr[i])\n",
    "        return np.array(merged)\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_segment_means(segments, values):\n",
    "        return [values[start:end].mean() if start < end else 0 for start, end in zip(segments[:-1], segments[1:])]\n",
    "\n",
    "    @staticmethod\n",
    "    def _rank_averages(averages):\n",
    "        sorted_indices = np.argsort(averages)[::-1]\n",
    "        ranks = np.zeros_like(sorted_indices) + 1\n",
    "        for rank, idx in enumerate(sorted_indices):\n",
    "            ranks[idx] = rank + 1\n",
    "        return ranks\n",
    "\n",
    "    def _extract_best_worst_segments(self, segments, ranks):\n",
    "        top_start, top_end = segments[np.where(ranks == 1)[0][0]], segments[np.where(ranks == 1)[0][0] + 1]\n",
    "        bottom_start, bottom_end = segments[np.where(ranks == max(ranks))[0][0]], segments[np.where(ranks == max(ranks))[0][0] + 1]\n",
    "        return (top_start, top_end), (bottom_start, bottom_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c312d3d",
   "metadata": {},
   "source": [
    "### Script Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ee01b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, processor):\n",
    "    video_batch, audio_batch = zip(*batch)\n",
    "    video_batch = [torch.tensor(v, dtype=torch.float32) for v in video_batch]\n",
    "    audio_batch = [torch.tensor(a, dtype=torch.float32) for a in audio_batch]\n",
    "    flow_ranks = [processor.get_of_ranks(video_batch[i], audio_batch[i]) for i in range(len(video_batch))]\n",
    "    return video_batch, audio_batch, flow_ranks\n",
    "\n",
    "def get_dataloader(path, batch_size=32, shuffle=True, method='video', window_size=20):\n",
    "    dataset = VideoAudioDataset(path)\n",
    "    processor = OpticalFlowProcessor(method=method, window_size=window_size)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=lambda batch: collate_fn(batch, processor))\n",
    "\n",
    "def perform_feature_padding(video_features, audio_features, start_segment, end_segment, max_seq_len):\n",
    "    vf = video_features.clone().detach()\n",
    "    af = audio_features.clone().detach()\n",
    "    vf =vf[start_segment:end_segment,:]\n",
    "    af = af[start_segment:end_segment,:]\n",
    "\n",
    "    pvf = torch.zeros(max_seq_len, 1024)\n",
    "    pvf[:vf.shape[0], :] = vf\n",
    "\n",
    "    paf = torch.zeros(max_seq_len, 128)\n",
    "    paf[:af.shape[0], :] = af\n",
    "\n",
    "    # Create mask (True for padding positions)\n",
    "    mask = torch.arange(max_seq_len) >= vf.shape[0]\n",
    "    mask = mask.unsqueeze(0)  # Convert to 2D (batch_size=1, seq_len)\n",
    "    return pvf, paf, mask\n",
    "\n",
    "# Function to find pairs with approximately equal differences\n",
    "def find_matching_index_pairs(array1, array2, tolerance=5):\n",
    "    # Calculate differences in array1 and array2\n",
    "    array1_diffs = np.diff(array1)\n",
    "    array2_diffs = np.diff(array2)\n",
    "\n",
    "    matching_pairs = []\n",
    "\n",
    "    # Loop through differences in array1\n",
    "    for i, diff1 in enumerate(array1_diffs):\n",
    "        # Find pairs of consecutive indices in array2 with similar differences\n",
    "        for j, diff2 in enumerate(array2_diffs):\n",
    "            if abs(diff1 - diff2) <= tolerance:  # If the difference is within the tolerance\n",
    "                matching_pairs.append(((i, i + 1), (j, j + 1), diff1, diff2))\n",
    "\n",
    "    return matching_pairs\n",
    "\n",
    "\n",
    "def get_similar_length_segments(positive_segments, negative_segments, tolerance = 5):\n",
    "    \n",
    "    while True:\n",
    "        matching_indexes = find_matching_index_pairs(positive_segments, negative_segments, tolerance=tolerance)\n",
    "        tolerance += 5\n",
    "        if len(matching_indexes) > 0:\n",
    "            break\n",
    "            \n",
    "    # sample randomly for all segments within the tolerance band\n",
    "    pos_segment, negative_segment, pos_time, neg_time = matching_indexes[np.random.randint(0, len(matching_indexes))]\n",
    "    \n",
    "    return pos_segment, negative_segment, pos_time, neg_time\n",
    "\n",
    "def get_positive_negative_embeddings(filenames, tolerance=5):\n",
    "\n",
    "    positive_record_file = np.random.choice(filenames)\n",
    "    negative_record_file = np.random.choice(filenames)\n",
    "\n",
    "    positive_rgb, positive_audio = sample_dataset(positive_record_file)\n",
    "    negative_rgb, negative_audio = sample_dataset(negative_record_file)\n",
    "\n",
    "    optical_flow_pos = calculate_optical_flow_euclidean(positive_rgb)\n",
    "    optical_flow_pos = moving_average(optical_flow_pos, window_size=20)\n",
    "\n",
    "    optical_flow_neg = calculate_optical_flow_euclidean(negative_rgb)\n",
    "    optical_flow_neg = moving_average(optical_flow_neg, window_size=20)\n",
    "\n",
    "    positive_segments = optical_flow_segments(optical_flow_pos)\n",
    "    negative_segments = optical_flow_segments(optical_flow_neg)\n",
    "\n",
    "    pos_segment, negative_segment, pos_time, neg_time = get_similar_length_segments(positive_segments, negative_segments, tolerance = 5)\n",
    "\n",
    "    # converting segment index to time in seconds\n",
    "    pos_start, pos_end = pos_segment\n",
    "    pos_start = positive_segments[pos_start]\n",
    "    pos_end = positive_segments[pos_end]\n",
    "    print(pos_start, pos_end)\n",
    "\n",
    "    # retrieving segment and padding it appropriately\n",
    "    pos_video, pos_audio, pos_mask = perform_feature_padding(positive_rgb, positive_audio, pos_start, pos_end, max_seq_len)\n",
    "\n",
    "    # computing embedding\n",
    "    positive_video_embedding = video_model(pos_video, pos_mask)\n",
    "    positive_audio_embedding = audio_model(pos_audio, pos_mask)\n",
    "\n",
    "    # converting segment index to time in seconds\n",
    "    neg_start, neg_end = negative_segment\n",
    "    neg_start = negative_segments[neg_start]\n",
    "    neg_end = negative_segments[neg_end]\n",
    "\n",
    "    # retrieving segment and padding it appropriately\n",
    "    neg_video, neg_audio, neg_mask = perform_feature_padding(negative_rgb, negative_audio, neg_start, neg_end, max_seq_len)\n",
    "\n",
    "    # computing embedding\n",
    "    negative_video_embedding = video_model(neg_video, neg_mask)\n",
    "    negative_audio_embedding = audio_model(neg_audio, neg_mask)\n",
    "    return positive_video_embedding, positive_audio_embedding, negative_video_embedding, negative_audio_embedding\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, filename):\n",
    "    \"\"\"Saves model and optimizer state dict.\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved at epoch {epoch} to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "8e511c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positive_negative_embeddings(video_batch, tolerance=5):\n",
    "\n",
    "    positive_record_file = np.random.choice(filenames)\n",
    "    negative_record_file = np.random.choice(filenames)\n",
    "\n",
    "    positive_rgb, positive_audio = sample_dataset(positive_record_file)\n",
    "    negative_rgb, negative_audio = sample_dataset(negative_record_file)\n",
    "\n",
    "    optical_flow_pos = calculate_optical_flow_euclidean(positive_rgb)\n",
    "    optical_flow_pos = moving_average(optical_flow_pos, window_size=20)\n",
    "\n",
    "    optical_flow_neg = calculate_optical_flow_euclidean(negative_rgb)\n",
    "    optical_flow_neg = moving_average(optical_flow_neg, window_size=20)\n",
    "\n",
    "    positive_segments = optical_flow_segments(optical_flow_pos)\n",
    "    negative_segments = optical_flow_segments(optical_flow_neg)\n",
    "\n",
    "    pos_segment, negative_segment, pos_time, neg_time = get_similar_length_segments(positive_segments, negative_segments, tolerance = 5)\n",
    "\n",
    "    # converting segment index to time in seconds\n",
    "    pos_start, pos_end = pos_segment\n",
    "    pos_start = positive_segments[pos_start]\n",
    "    pos_end = positive_segments[pos_end]\n",
    "    print(pos_start, pos_end)\n",
    "\n",
    "    # retrieving segment and padding it appropriately\n",
    "    pos_video, pos_audio, pos_mask = perform_feature_padding(positive_rgb, positive_audio, pos_start, pos_end, max_seq_len)\n",
    "\n",
    "    # computing embedding\n",
    "    positive_video_embedding = video_model(pos_video, pos_mask)\n",
    "    positive_audio_embedding = audio_model(pos_audio, pos_mask)\n",
    "\n",
    "    # converting segment index to time in seconds\n",
    "    neg_start, neg_end = negative_segment\n",
    "    neg_start = negative_segments[neg_start]\n",
    "    neg_end = negative_segments[neg_end]\n",
    "\n",
    "    # retrieving segment and padding it appropriately\n",
    "    neg_video, neg_audio, neg_mask = perform_feature_padding(negative_rgb, negative_audio, neg_start, neg_end, max_seq_len)\n",
    "\n",
    "    # computing embedding\n",
    "    negative_video_embedding = video_model(neg_video, neg_mask)\n",
    "    negative_audio_embedding = audio_model(neg_audio, neg_mask)\n",
    "    return positive_video_embedding, positive_audio_embedding, negative_video_embedding, negative_audio_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "af74f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 100\n",
    "\n",
    "\n",
    "def get_segmentd_embeddings(video_model, audio_model, vid, aud):\n",
    "    vid_segment_embeddings = []\n",
    "    \n",
    "\n",
    "    flow = OpticalFlowProcessor()._compute_flow(vid, aud)\n",
    "    segments = OpticalFlowProcessor()._optical_flow_segments(flow)\n",
    "\n",
    "    vid_segment_embeddings = []\n",
    "    aud_segment_embeddings = []\n",
    "    for i in range(1, len(segments)):\n",
    "        start = segments[i-1]\n",
    "        end = segments[i]\n",
    "\n",
    "        vid_emb, aud_emb, mask = perform_feature_padding(vid, aud, start, end, max_seq_len)\n",
    "        \n",
    "        vid_segment_embeddings.append(video_model(vid_emb, mask))\n",
    "        aud_segment_embeddings.append(audio_model(aud_emb, mask))\n",
    "    return vid_segment_embeddings, aud_segment_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "6ac618a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads =1\n",
    "num_layers=1\n",
    "audio_model = Transformer(input_dim=128, embed_dim=256, num_heads=num_heads, num_layers=num_layers, max_seq_len=max_seq_len)\n",
    "video_model = Transformer(input_dim=1024, embed_dim=256, num_heads=num_heads, num_layers=num_layers, max_seq_len=max_seq_len)\n",
    "\n",
    "def get_batch_embeddings(video_model, audio_model, video_batch, audio_batch):\n",
    "    # We precompute the segment embeddings in each batch.  We do this once and then proceed to processing batch\n",
    "    batch_vid_embeddings = []\n",
    "    batch_aud_embeddings = []\n",
    "    for i in range(len(video_batch)):\n",
    "        vid = video_batch[i]\n",
    "        aud = audio_batch[i]\n",
    "        vid_sgmt_emb, aud_sgmt_emb = get_segmentd_embeddings(video_model, audio_model, vid, aud)\n",
    "        batch_vid_embeddings.extend(vid_sgmt_emb)\n",
    "        batch_aud_embeddings.extend(aud_sgmt_emb)\n",
    "        \n",
    "    # Shape will by (total segments X embedding dim)\n",
    "    # total segments is clip dependent\n",
    "    batch_aud_embeddings = torch.stack(batch_aud_embeddings)\n",
    "    batch_vid_embeddings = torch.stack(batch_vid_embeddings)\n",
    "    \n",
    "    # MAKE SURE VECTORS ARE NORMALIZED FIRST idk if I want to do here or later..\n",
    "    batch_aud_embeddings = torch.nn.functional.normalize(batch_aud_embeddings, p=2, dim=1)\n",
    "    batch_vid_embeddings = torch.nn.functional.normalize(batch_vid_embeddings, p=2, dim=1)\n",
    "\n",
    "    return batch_aud_embeddings, batch_vid_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "e4494c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intermodal_loss(batch_vid_embeddings, batch_aud_embeddings, k=5, min_val=0):\n",
    "    # batch_vid_embeddings and  batch_aud_embeddings should already be normalized so \n",
    "    # multiplying them is a similarity metric\n",
    "\n",
    "    # convert simliarity to distance by (-1) >> high value indicates the distance between the samples is long\n",
    "    dist_xy = (-1) *torch.matmul(batch_vid_embeddings, batch_aud_embeddings.T)\n",
    "\n",
    "    positive_pairs = torch.diag(dist_xy)\n",
    "\n",
    "    # Get non-diagonal elements (negative examples)\n",
    "    # First, create a mask for non-diagonal elements\n",
    "    mask = ~torch.eye(dist_xy.size(0), dtype=torch.bool)\n",
    "\n",
    "    # Apply the mask to extract non-diagonal elements\n",
    "    negative_pairs = dist_xy[mask]\n",
    "\n",
    "    # First we find the positive pairs that are furthest in embedding space\n",
    "    topk_pos_values, _ = torch.topk(positive_pairs.flatten(), k, largest=True)\n",
    "\n",
    "    # next we find the negative pairs that are closest in embedding space\n",
    "    topk_neg_values, _ = torch.topk(negative_pairs.flatten(), k, largest=False)\n",
    "\n",
    "    # expand so we compare all possible combinations of pos/neg pairs\n",
    "    topk_pos_values_expanded = topk_pos_values.unsqueeze(1)  # Shape: (k, 1)\n",
    "    topk_neg_values_expanded = topk_neg_values.unsqueeze(0)  # Shape: (1, k)\n",
    "    loss = torch.maximum(torch.tensor(min_value), topk_pos_values_expanded - topk_neg_values_expanded)\n",
    "    loss = loss.mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f5d801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95b6477a",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "b2b7991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69481f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "39c06cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda1 = 0.33\n",
    "lambda2 = 0.33\n",
    "lambda3 = 0.33\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "# Define the Adam optimizer for the audio model\n",
    "audio_optimizer = optim.Adam(audio_model.parameters(), lr=lr)\n",
    "\n",
    "# Define the Adam optimizer for the video model\n",
    "video_optimizer = optim.Adam(video_model.parameters(), lr=lr)\n",
    "\n",
    "batch_size = 10\n",
    "window_size = 20 # for optical flow smoothing (ie 20 frame mavg flow)\n",
    "\n",
    "dataloader = get_dataloader(path, batch_size=batch_size, shuffle=True, method='video', window_size=window_size)\n",
    "num_flow_matching = 10\n",
    "k = 20\n",
    "margin = 0.1\n",
    "triplet_loss = nn.TripletMarginLoss(margin=margin, p=4, eps=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "48de3380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch iterator\n",
    "for video_batch, audio_batch, flow_ranks in dataloader:\n",
    "    try:\n",
    "        audio_optimizer.zero_grad()\n",
    "        video_optimizer.zero_grad()\n",
    "        total_loss = 0\n",
    "\n",
    "        # create segments for each batch and compute embeddings for the segments\n",
    "        # stack all the embeddings into single tensors\n",
    "        batch_aud_embeddings, batch_vid_embeddings = get_batch_embeddings(video_model, audio_model, video_batch, audio_batch)\n",
    "\n",
    "        \n",
    "        # 1. Inter-modal loss\n",
    "        inter_modal_loss = get_intermodal_loss(batch_vid_embeddings, batch_aud_embeddings, k=5, min_val=0)\n",
    "\n",
    "\n",
    "        # 2. optical flow loss\n",
    "\n",
    "        # this code finds the top and bottom ranked optical flow for a particular\n",
    "        # video.  This is specified in flow ranks.  It then converts these indexes to \n",
    "        # their corresponding position in the stacked embeddings\n",
    "        top_rank_idxs = []\n",
    "        bottom_rank_idxs = []\n",
    "        current_index = 0\n",
    "        for ranks in flow_ranks:\n",
    "            top_rank_idxs.append(current_index + np.argmin(ranks))\n",
    "            bottom_rank_idxs.append(current_index + np.argmax(ranks))\n",
    "            current_index += len(ranks)\n",
    "\n",
    "        # Randomly choose num_flow_matching pairs to match (top_rank, top_rank, bottom rank)\n",
    "        top_matching_samples = list(itertools.product(top_rank_idxs, top_rank_idxs, bottom_rank_idxs))\n",
    "        top_matching_samples = [random.choice(top_matching_samples) for _ in range(num_flow_matching)]\n",
    "\n",
    "        # Randomly choose num_flow_matching pairs to match (bottom, bottom, top_rank rank)\n",
    "        bottom_matching_samples = list(itertools.product(bottom_rank_idxs, bottom_rank_idxs, top_rank_idxs))\n",
    "        bottom_matching_samples = [random.choice(bottom_matching_samples) for _ in range(num_flow_matching)]\n",
    "\n",
    "        of_loss_top = 0\n",
    "        for anchor, pos, neg in top_matching_samples:\n",
    "            of_loss_top += triplet_loss(batch_vid_embeddings[anchor], batch_vid_embeddings[pos], batch_vid_embeddings[neg])\n",
    "            of_loss_top += triplet_loss(batch_aud_embeddings[anchor], batch_aud_embeddings[pos], batch_aud_embeddings[neg])\n",
    "\n",
    "        of_loss_bottom = 0\n",
    "        for anchor, pos, neg in bottom_matching_samples:\n",
    "            of_loss_bottom += triplet_loss(batch_vid_embeddings[anchor], batch_vid_embeddings[pos], batch_vid_embeddings[neg])\n",
    "            of_loss_bottom += triplet_loss(batch_aud_embeddings[anchor], batch_aud_embeddings[pos], batch_aud_embeddings[neg])\n",
    "\n",
    "        loss = lambda1*inter_modal_loss + lambda2*of_loss_top + lambda3*of_loss_bottom\n",
    "        loss.backward()\n",
    "        audio_optimizer.step()\n",
    "        video_optimizer.step()\n",
    "        \n",
    "    except Exception as e:\n",
    "        # adding a wrapper just in case\n",
    "        print(e)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "9c03fa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_batch, audio_batch, flow_ranks in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab13eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f109d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_aud_embeddings, batch_vid_embeddings = get_batch_embeddings(video_model, audio_model, video_batch, audio_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665da346",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "min_val = 0\n",
    "# batch_vid_embeddings and  batch_aud_embeddings should already be normalized so \n",
    "# multiplying them is a similarity metric\n",
    "\n",
    "# convert simliarity to distance by (-1) >> high value indicates the distance between the samples is long\n",
    "dist_xy = (-1) *torch.matmul(batch_vid_embeddings, batch_aud_embeddings.T)\n",
    "\n",
    "positive_pairs = torch.diag(dist_xy)\n",
    "\n",
    "# Get non-diagonal elements (negative examples)\n",
    "# First, create a mask for non-diagonal elements\n",
    "mask = ~torch.eye(dist_xy.size(0), dtype=torch.bool)\n",
    "\n",
    "# Apply the mask to extract non-diagonal elements\n",
    "negative_pairs = dist_xy[mask]\n",
    "\n",
    "# top k positive and negative pairs\n",
    "topk_pos_values, _ = torch.topk(positive_pairs.flatten(), k)\n",
    "topk_neg_values, _ = torch.topk(negative_pairs.flatten(), k)\n",
    "\n",
    "#\n",
    "topk_pos_values_expanded = topk_pos_values.unsqueeze(1)  # Shape: (3, 1)\n",
    "topk_neg_values_expanded = topk_neg_values.unsqueeze(0)  # Shape: (1, 3)\n",
    "loss = torch.maximum(torch.tensor(0.0), topk_pos_values_expanded - topk_neg_values_expanded)\n",
    "loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac22d5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2096a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_neg_pair_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "177389c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(13, 13), dtype=float32, numpy=\n",
       "array([[-3.3602990e-02, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06],\n",
       "       [-1.0000000e+06, -4.2709179e-02, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06],\n",
       "       [-1.0000000e+06, -1.0000000e+06, -2.5183383e-02, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06],\n",
       "       [-1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -3.7141740e-02,\n",
       "        -1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06],\n",
       "       [-1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -3.5998885e-02, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06],\n",
       "       [-1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.7300233e-02, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06],\n",
       "       [-1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.0000000e+06, -2.6054785e-02, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06],\n",
       "       [-1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -4.9366400e-02,\n",
       "        -1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06],\n",
       "       [-1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -4.5822185e-02, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06],\n",
       "       [-1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06, -4.0678170e-02, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06],\n",
       "       [-1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.0000000e+06, -3.6392711e-02, -1.0000000e+06,\n",
       "        -1.0000000e+06],\n",
       "       [-1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -3.5263870e-02,\n",
       "        -1.0000000e+06],\n",
       "       [-1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -1.0000000e+06, -1.0000000e+06, -1.0000000e+06, -1.0000000e+06,\n",
       "        -4.6147674e-02]], dtype=float32)>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_pos_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "26fa8282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert simliarity to distance by (-1) >> high value indicates the distance between the samples is long\n",
    "dist_xy = (-1) *torch.matmul(batch_vid_embeddings, batch_aud_embeddings.T)\n",
    "\n",
    "positive_pairs = torch.diag(dist_xy)\n",
    "\n",
    "# Get non-diagonal elements (negative examples)\n",
    "# First, create a mask for non-diagonal elements\n",
    "mask = ~torch.eye(dist_xy.size(0), dtype=torch.bool)\n",
    "\n",
    "# Apply the mask to extract non-diagonal elements\n",
    "negative_pairs = dist_xy[mask]\n",
    "\n",
    "# First we find the positive pairs that are furthest in embedding space\n",
    "topk_pos_values, _ = torch.topk(positive_pairs.flatten(), k, largest=True)\n",
    "\n",
    "# next we find the negative pairs that are closest in embedding space\n",
    "topk_neg_values, _ = torch.topk(negative_pairs.flatten(), k, largest=False)\n",
    "\n",
    "# expand so we compare all possible combinations of pos/neg pairs\n",
    "topk_pos_values_expanded = topk_pos_values.unsqueeze(1)  # Shape: (k, 1)\n",
    "topk_neg_values_expanded = topk_neg_values.unsqueeze(0)  # Shape: (1, k)\n",
    "loss = torch.maximum(torch.tensor(min_value), topk_pos_values_expanded - topk_neg_values_expanded)\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "5c2650cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0671, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "0c2b47e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0460, 0.0485, 0.0492, 0.0504, 0.0513, 0.0522, 0.0524, 0.0524, 0.0525,\n",
       "        0.0540, 0.0542, 0.0548, 0.0556, 0.0557, 0.0561, 0.0562, 0.0563, 0.0564,\n",
       "        0.0564, 0.0564], grad_fn=<TopkBackward0>)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_neg_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8552b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a56c103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d90d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a519adc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "109f14e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topk__values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[320], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtopk__values\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'topk__values' is not defined"
     ]
    }
   ],
   "source": [
    "topk__values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "78781e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027399999999999997"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.0261-( -0.0535)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "3a8897e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__SelectV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} condition [13,13], then [100,100], and else [100,100] must be broadcastable [Op:SelectV2] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[322], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m dist_xy \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmatmul(batch_vid_embeddings, batch_aud_embeddings\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m      3\u001b[0m dist_xy \u001b[38;5;241m=\u001b[39m dist_xy\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m----> 4\u001b[0m dist_pos_pair \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43maff_xy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist_xy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_xy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1e+6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m dist_neg_pair \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mwhere(tf\u001b[38;5;241m.\u001b[39mlogical_not(aff_xy), dist_xy, tf\u001b[38;5;241m.\u001b[39mones_like(dist_xy, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1e+6\u001b[39m))\n\u001b[1;32m      7\u001b[0m top_k_pos_pair_xy, _ \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mtop_k(dist_pos_pair, k)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/coingame/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/coingame/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__SelectV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} condition [13,13], then [100,100], and else [100,100] must be broadcastable [Op:SelectV2] name: "
     ]
    }
   ],
   "source": [
    "#dist_xy = dist_xy.deatch().numpy()\n",
    "dist_xy = (-1) *torch.matmul(batch_vid_embeddings, batch_aud_embeddings.T)\n",
    "dist_xy = dist_xy.detach().numpy()\n",
    "dist_pos_pair = tf.where(aff_xy, dist_xy, tf.ones_like(dist_xy, dtype=tf.float32) * (-1e+6))\n",
    "dist_neg_pair = tf.where(tf.logical_not(aff_xy), dist_xy, tf.ones_like(dist_xy, dtype=tf.float32) * (1e+6))\n",
    "\n",
    "top_k_pos_pair_xy, _ = tf.nn.top_k(dist_pos_pair, k)\n",
    "top_k_pos_pair_yx, _ = tf.nn.top_k(tf.transpose(dist_pos_pair), k)\n",
    "\n",
    "top_k_neg_pair_xy, _ = tf.nn.top_k(tf.negative(dist_neg_pair), k=k)\n",
    "top_k_neg_pair_yx, _ = tf.nn.top_k(tf.transpose(tf.negative(dist_neg_pair)), k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "b576ddef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.1260, 0.1260, 0.1246, 0.1228, 0.1220, 0.1218, 0.1208, 0.1208, 0.1202,\n",
       "        0.1201, 0.1199, 0.1191, 0.1190, 0.1187, 0.1185, 0.1185, 0.1184, 0.1180,\n",
       "        0.1168, 0.1168], grad_fn=<TopkBackward0>),\n",
       "indices=tensor([73, 79, 42, 43, 74, 21, 45, 32, 92, 36, 44, 33, 14, 29, 96, 31, 37, 10,\n",
       "         6, 57]))"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(positive_pairs.flatten(), k, largest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "347e539c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(13, 3), dtype=float32, numpy=\n",
       "array([[-3.3602990e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-4.2709179e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-2.5183383e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-3.7141740e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-3.5998885e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-1.7300233e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-2.6054785e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-4.9366400e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-4.5822185e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-4.0678170e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-3.6392711e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-3.5263870e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-4.6147674e-02, -1.0000000e+06, -1.0000000e+06]], dtype=float32)>"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_pos_pair_yx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "3fa6e353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(13, 3), dtype=float32, numpy=\n",
       "array([[-3.3602990e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-4.2709179e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-2.5183383e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-3.7141740e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-3.5998885e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-1.7300233e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-2.6054785e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-4.9366400e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-4.5822185e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-4.0678170e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-3.6392711e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-3.5263870e-02, -1.0000000e+06, -1.0000000e+06],\n",
       "       [-4.6147674e-02, -1.0000000e+06, -1.0000000e+06]], dtype=float32)>"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_pos_pair_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "6b70e0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(13, 3), dtype=float32, numpy=\n",
       "array([[0.05375832, 0.05159993, 0.05075669],\n",
       "       [0.05293407, 0.05169568, 0.05119767],\n",
       "       [0.05683632, 0.05351436, 0.05201124],\n",
       "       [0.04753361, 0.04561421, 0.04552523],\n",
       "       [0.05258773, 0.05053359, 0.04976318],\n",
       "       [0.02967066, 0.02767048, 0.02642583],\n",
       "       [0.0351287 , 0.03366867, 0.0335088 ],\n",
       "       [0.05294877, 0.05142174, 0.05061736],\n",
       "       [0.05047916, 0.04883974, 0.0488084 ],\n",
       "       [0.04248676, 0.0408433 , 0.03894409],\n",
       "       [0.04294197, 0.04116744, 0.04040598],\n",
       "       [0.03703857, 0.0348169 , 0.03311012],\n",
       "       [0.04446257, 0.04444519, 0.04253142]], dtype=float32)>"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_neg_pair_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "b9cbc9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(13, 3), dtype=float32, numpy=\n",
       "array([[0.03689217, 0.03509057, 0.03457905],\n",
       "       [0.04213858, 0.04175154, 0.04088757],\n",
       "       [0.02850987, 0.02848868, 0.02655753],\n",
       "       [0.04275013, 0.04201677, 0.041509  ],\n",
       "       [0.04033705, 0.03785755, 0.03764169],\n",
       "       [0.04292034, 0.04264598, 0.04182005],\n",
       "       [0.04283515, 0.04231308, 0.04121768],\n",
       "       [0.05102314, 0.04979153, 0.04940118],\n",
       "       [0.04943028, 0.04871679, 0.04826463],\n",
       "       [0.05351436, 0.05169568, 0.05159993],\n",
       "       [0.04795438, 0.04740145, 0.04738085],\n",
       "       [0.05201124, 0.05119767, 0.05075669],\n",
       "       [0.05683632, 0.05375832, 0.05294877]], dtype=float32)>"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_neg_pair_yx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "a9154d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0568, -0.0538, -0.0535], grad_fn=<TopkBackward0>)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_neg_values2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "2cac5fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "dataloader = get_dataloader(path, batch_size=batch_size, shuffle=True, method='video', window_size=window_size)\n",
    "\n",
    "for video_batch, audio_batch, flow_ranks in dataloader:\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9476f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "29f18366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test we need to conver these segmented embeddings to \n",
    "# stacked segmented emebeddings each corresponding to a single video/audio\n",
    "batch_vid_embeddings_long = batch_vid_embeddings.reshape(batch_size, -1)\n",
    "batch_aud_embeddings_long = batch_aud_embeddings.reshape(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "020be037",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = torch.matmul(batch_vid_embeddings_long, batch_aud_embeddings_long.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "94101243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 20])"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "e18b904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_recall1(similarity_matrix, k):\n",
    "    \"\"\"\n",
    "    Compute top-k recall from a similarity matrix.\n",
    "    \n",
    "    similarity_matrix: Tensor of shape (batch_size, batch_size), where\n",
    "                       diagonal elements represent positive pairs.\n",
    "    k: The value of k for top-k recall.\n",
    "    \n",
    "    Returns:\n",
    "        recall: The top-k recall for each row (video or audio example).\n",
    "    \"\"\"\n",
    "    batch_size = similarity_matrix.size(0)\n",
    "    \n",
    "    # Create a mask for the diagonal (positive examples)\n",
    "    diagonal_mask = torch.eye(batch_size, device=similarity_matrix.device)\n",
    "    \n",
    "    # We will calculate recall for each row (video or audio)\n",
    "    recall_list = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Get the similarity values for the current row\n",
    "        row_similarities = similarity_matrix[i]\n",
    "        \n",
    "        # Find the top K indices based on similarity (excluding the diagonal element)\n",
    "        _, top_k_indices = torch.topk(row_similarities, k)\n",
    "        \n",
    "        # Check if the true positive (diagonal element) is in the top-k indices\n",
    "        # The diagonal element is at index (i, i)\n",
    "        true_positive_idx = i\n",
    "        \n",
    "        # If the true positive is in the top K, add 1 to recall\n",
    "        if true_positive_idx in top_k_indices:\n",
    "            recall_list.append(1.0)\n",
    "        else:\n",
    "            recall_list.append(0.0)\n",
    "    \n",
    "    # Return the mean recall across all examples (rows)\n",
    "    recall = torch.mean(torch.tensor(recall_list, device=similarity_matrix.device))\n",
    "    return recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "c1e80331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_recall(similarity_matrix, k):\n",
    "    \"\"\"\n",
    "    Compute top-k recall from a similarity matrix (vectorized version).\n",
    "    \n",
    "    similarity_matrix: Tensor of shape (batch_size, batch_size), where\n",
    "                       diagonal elements represent positive pairs.\n",
    "    k: The value of k for top-k recall.\n",
    "    \n",
    "    Returns:\n",
    "        recall: The top-k recall (mean recall across all examples).\n",
    "    \"\"\"\n",
    "    batch_size = similarity_matrix.size(0)\n",
    "\n",
    "    # Get the indices of the top-k most similar items for each row (video/audio example)\n",
    "    _, top_k_indices = torch.topk(similarity_matrix, k, dim=1)\n",
    "\n",
    "    # Create a tensor for diagonal indices (i, i) for each row\n",
    "    diagonal_indices = torch.arange(batch_size, device=similarity_matrix.device)\n",
    "\n",
    "    # Check if the diagonal index of each row is in the top-K indices of that row\n",
    "    # `top_k_indices` is of shape (batch_size, k)\n",
    "    is_true_positive_in_top_k = (top_k_indices == diagonal_indices.unsqueeze(1))\n",
    "\n",
    "    # Calculate recall: for each row, check if the diagonal index is in the top-K\n",
    "    recall_per_row = is_true_positive_in_top_k.any(dim=1).float()\n",
    "\n",
    "    # Return the mean recall across all examples (rows)\n",
    "    return recall_per_row.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "5af4215e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0500)"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_recall2(similarity_matrix, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "3144d4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0500)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_recall1(similarity_matrix, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "cf0dbc80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03125"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "8621af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_and_covariance(embeddings):\n",
    "    \"\"\"\n",
    "    Computes the mean and covariance matrix of the embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (torch.Tensor): A tensor of shape (num_samples, feature_dim)\n",
    "    \n",
    "    Returns:\n",
    "        mean (torch.Tensor): The mean of the embeddings.\n",
    "        covariance (torch.Tensor): The covariance matrix of the embeddings.\n",
    "    \"\"\"\n",
    "    mean = embeddings.mean(dim=0)\n",
    "    centered_embeddings = embeddings - mean\n",
    "    covariance = torch.matmul(centered_embeddings.T, centered_embeddings) / (embeddings.size(0) - 1)\n",
    "    return mean, covariance\n",
    "\n",
    "def calculate_frechet_audio_distance(embeddings_ground_truth, embeddings_retrieved, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Calculates the Fréchet Audio Distance (FAD) between two sets of embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embeddings_ground_truth (torch.Tensor): Ground truth audio embeddings (num_samples, feature_dim).\n",
    "        embeddings_retrieved (torch.Tensor): Retrieved audio embeddings (num_samples, feature_dim).\n",
    "        epsilon (float): Regularization term to ensure positive semi-definiteness of covariance matrices.\n",
    "        \n",
    "    Returns:\n",
    "        fad (float): The Fréchet Audio Distance between the two sets of embeddings.\n",
    "    \"\"\"\n",
    "    # Compute the mean and covariance for ground truth and retrieved embeddings\n",
    "    mu_x, sigma_x = compute_mean_and_covariance(embeddings_ground_truth)\n",
    "    mu_y, sigma_y = compute_mean_and_covariance(embeddings_retrieved)\n",
    "\n",
    "    # Calculate the term: ||mu_x - mu_y||^2\n",
    "    diff_mu = mu_x - mu_y\n",
    "    mu_term = torch.sum(diff_mu ** 2)\n",
    "\n",
    "    # Add epsilon to the covariance matrices to ensure they are positive semi-definite\n",
    "    sigma_x += epsilon * torch.eye(sigma_x.size(0), device=sigma_x.device)\n",
    "    sigma_y += epsilon * torch.eye(sigma_y.size(0), device=sigma_y.device)\n",
    "\n",
    "    # Perform Cholesky decomposition to compute the matrix square root of covariance matrices\n",
    "    try:\n",
    "        sigma_x_sqrt = torch.linalg.cholesky(sigma_x)\n",
    "        sigma_y_sqrt = torch.linalg.cholesky(sigma_y)\n",
    "    except RuntimeError:\n",
    "        raise ValueError(\"Covariance matrices are not positive semi-definite, even with regularization\")\n",
    "        return float('inf')\n",
    "    # Calculate the term: Tr(sigma_x + sigma_y - 2(sigma_x^0.5 * sigma_y * sigma_x^0.5)^0.5)\n",
    "    term = torch.trace(sigma_x + sigma_y - 2 * torch.matmul(sigma_x_sqrt, torch.matmul(sigma_y_sqrt, sigma_x_sqrt.T)))\n",
    "\n",
    "    # The FAD is the sum of the two terms\n",
    "    fad = mu_term + term\n",
    "    return fad.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "17db09ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the maximum values along each row (dim=1)\n",
    "_, most_similar_indices = torch.max(similarity_matrix, dim=1)\n",
    "retrieved_audio_embeddings = batch_aud_embeddings_long[most_similar_indices]\n",
    "calculate_frechet_audio_distance(batch_aud_embeddings_long, retrieved_audio_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "b79b89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_audio_embeddings = batch_aud_embeddings_long[most_similar_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "ebc1b50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3900473713874817"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_frechet_audio_distance(batch_aud_embeddings_long, retrieved_audio_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "0c5b1209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "#from scipy.signal import find_peaks\n",
    "#from sklearn.metrics import peak_prominences\n",
    "import itertools\n",
    "\n",
    "# Function to detect local maxima in embeddings using scipy's find_peaks\n",
    "def find_local_maxima_in_embeddings(embeddings, prominence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Detect local maxima in the embeddings using scipy's find_peaks and sklearn's peak_prominences.\n",
    "\n",
    "    Args:\n",
    "        embeddings (torch.Tensor): Tensor of shape (batch_size, feature_dim).\n",
    "        prominence_threshold (float): Minimum prominence required to consider a peak.\n",
    "\n",
    "    Returns:\n",
    "        peaks_list (list): List of indices where the local maxima (peaks) occur in the embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = embeddings.cpu().detach().numpy()  # Convert to numpy for peak detection\n",
    "\n",
    "    peaks_list = []\n",
    "    for i in range(embeddings.shape[0]):  # Iterate through each embedding (e.g., audio or video)\n",
    "        # Find local maxima (peaks) in the embedding\n",
    "        peaks, _ = find_peaks(embeddings[i])\n",
    "        prominences = peak_prominences(embeddings[i], peaks)[0]\n",
    "        \n",
    "        # Filter peaks based on prominence\n",
    "        significant_peaks = peaks[prominences >= prominence_threshold]\n",
    "        peaks_list.append(significant_peaks)\n",
    "\n",
    "    return peaks_list\n",
    "\n",
    "\n",
    "# Function to calculate Intersection over Union (IoU) between audio and video peaks\n",
    "def calc_intersection_over_union(audio_peaks, video_peaks):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU) between the audio and video peaks.\n",
    "\n",
    "    Args:\n",
    "        audio_peaks (list): Indices of audio peaks.\n",
    "        video_peaks (list): Indices of video peaks.\n",
    "\n",
    "    Returns:\n",
    "        float: IoU score between audio and video peaks.\n",
    "    \"\"\"\n",
    "    intersection = len(set(audio_peaks).intersection(set(video_peaks)))\n",
    "    union = len(set(audio_peaks).union(set(video_peaks)))\n",
    "    iou_score = intersection / union\n",
    "    return iou_score\n",
    "\n",
    "\n",
    "# Function to compute the AV-Align score from audio and video embeddings\n",
    "def compute_av_align_score(audio_embeddings, video_embeddings, prominence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute the AV-Align score between the audio and video embeddings.\n",
    "\n",
    "    Args:\n",
    "        audio_embeddings (torch.Tensor): Audio embeddings of shape (batch_size, feature_dim).\n",
    "        video_embeddings (torch.Tensor): Video embeddings of shape (batch_size, feature_dim).\n",
    "        prominence_threshold (float): Minimum prominence to filter significant peaks.\n",
    "\n",
    "    Returns:\n",
    "        float: AV-Align score (IoU between audio and video peaks).\n",
    "    \"\"\"\n",
    "    # Detect peaks in the audio and video embeddings\n",
    "    audio_peaks = find_local_maxima_in_embeddings(audio_embeddings, prominence_threshold)\n",
    "    video_peaks = find_local_maxima_in_embeddings(video_embeddings, prominence_threshold)\n",
    "\n",
    "    # Flatten the list of peaks before calculating the IoU\n",
    "    audio_peaks_flattened = list(itertools.chain(*audio_peaks))\n",
    "    video_peaks_flattened = list(itertools.chain(*video_peaks))\n",
    "\n",
    "    # Calculate the Intersection over Union (IoU) for the audio and video peaks\n",
    "    iou_score = calc_intersection_over_union(audio_peaks_flattened, video_peaks_flattened)\n",
    "    \n",
    "    return iou_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "4f980855",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_peaks = find_local_maxima_in_embeddings(retrieved_audio_embeddings, prominence_threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "f9449b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10040983606557377"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_av_align_score(batch_vid_embeddings_long, retrieved_audio_embeddings, prominence_threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "0b05abeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_av_align_score(batch_vid_embeddings_long, batch_aud_embeddings_long, prominence_threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "77d8fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "class Eval():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Function to detect local maxima in embeddings using scipy's find_peaks\n",
    "    def find_local_maxima_in_embeddings(self, embeddings, prominence_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Detect local maxima in the embeddings using scipy's find_peaks and sklearn's peak_prominences.\n",
    "\n",
    "        Args:\n",
    "            embeddings (torch.Tensor): Tensor of shape (batch_size, feature_dim).\n",
    "            prominence_threshold (float): Minimum prominence required to consider a peak.\n",
    "\n",
    "        Returns:\n",
    "            peaks_list (list): List of indices where the local maxima (peaks) occur in the embeddings.\n",
    "        \"\"\"\n",
    "        embeddings = embeddings.cpu().detach().numpy()  # Convert to numpy for peak detection\n",
    "\n",
    "        peaks_list = []\n",
    "        for i in range(embeddings.shape[0]):  # Iterate through each embedding (e.g., audio or video)\n",
    "            # Find local maxima (peaks) in the embedding\n",
    "            peaks, _ = find_peaks(embeddings[i])\n",
    "            prominences = peak_prominences(embeddings[i], peaks)[0]\n",
    "\n",
    "            # Filter peaks based on prominence\n",
    "            significant_peaks = peaks[prominences >= prominence_threshold]\n",
    "            peaks_list.append(significant_peaks)\n",
    "\n",
    "        return peaks_list\n",
    "\n",
    "    # Function to calculate Intersection over Union (IoU) between audio and video peaks\n",
    "    def calc_intersection_over_union(self, audio_peaks, video_peaks):\n",
    "        \"\"\"\n",
    "        Calculate Intersection over Union (IoU) between the audio and video peaks.\n",
    "\n",
    "        Args:\n",
    "            audio_peaks (list): Indices of audio peaks.\n",
    "            video_peaks (list): Indices of video peaks.\n",
    "\n",
    "        Returns:\n",
    "            float: IoU score between audio and video peaks.\n",
    "        \"\"\"\n",
    "        intersection = len(set(audio_peaks).intersection(set(video_peaks)))\n",
    "        union = len(set(audio_peaks).union(set(video_peaks)))\n",
    "        iou_score = intersection / union\n",
    "        return iou_score\n",
    "\n",
    "    # Function to compute the AV-Align score from audio and video embeddings\n",
    "    def compute_av_align_score(self, audio_embeddings, video_embeddings, prominence_threshold=0.1):\n",
    "        \"\"\"\n",
    "        Compute the AV-Align score between the audio and video embeddings.\n",
    "\n",
    "        Args:\n",
    "            audio_embeddings (torch.Tensor): Audio embeddings of shape (batch_size, feature_dim).\n",
    "            video_embeddings (torch.Tensor): Video embeddings of shape (batch_size, feature_dim).\n",
    "            prominence_threshold (float): Minimum prominence to filter significant peaks.\n",
    "\n",
    "        Returns:\n",
    "            float: AV-Align score (IoU between audio and video peaks).\n",
    "        \"\"\"\n",
    "        # Detect peaks in the audio and video embeddings\n",
    "        audio_peaks = self.find_local_maxima_in_embeddings(audio_embeddings, prominence_threshold)\n",
    "        video_peaks = self.find_local_maxima_in_embeddings(video_embeddings, prominence_threshold)\n",
    "\n",
    "        # Flatten the list of peaks before calculating the IoU\n",
    "        audio_peaks_flattened = list(itertools.chain(*audio_peaks))\n",
    "        video_peaks_flattened = list(itertools.chain(*video_peaks))\n",
    "\n",
    "        # Calculate the Intersection over Union (IoU) for the audio and video peaks\n",
    "        iou_score = self.calc_intersection_over_union(audio_peaks_flattened, video_peaks_flattened)\n",
    "\n",
    "        return iou_score\n",
    "\n",
    "    def compute_mean_and_covariance(self, embeddings):\n",
    "        \"\"\"\n",
    "        Computes the mean and covariance matrix of the embeddings.\n",
    "\n",
    "        Args:\n",
    "            embeddings (torch.Tensor): A tensor of shape (num_samples, feature_dim)\n",
    "\n",
    "        Returns:\n",
    "            mean (torch.Tensor): The mean of the embeddings.\n",
    "            covariance (torch.Tensor): The covariance matrix of the embeddings.\n",
    "        \"\"\"\n",
    "        mean = embeddings.mean(dim=0)\n",
    "        centered_embeddings = embeddings - mean\n",
    "        covariance = torch.matmul(centered_embeddings.T, centered_embeddings) / (embeddings.size(0) - 1)\n",
    "        return mean, covariance\n",
    "\n",
    "    def calculate_frechet_audio_distance(self, embeddings_ground_truth, embeddings_retrieved, epsilon=1e-6):\n",
    "        \"\"\"\n",
    "        Calculates the Fréchet Audio Distance (FAD) between two sets of embeddings.\n",
    "\n",
    "        Args:\n",
    "            embeddings_ground_truth (torch.Tensor): Ground truth audio embeddings (num_samples, feature_dim).\n",
    "            embeddings_retrieved (torch.Tensor): Retrieved audio embeddings (num_samples, feature_dim).\n",
    "            epsilon (float): Regularization term to ensure positive semi-definiteness of covariance matrices.\n",
    "\n",
    "        Returns:\n",
    "            fad (float): The Fréchet Audio Distance between the two sets of embeddings.\n",
    "        \"\"\"\n",
    "        # Compute the mean and covariance for ground truth and retrieved embeddings\n",
    "        mu_x, sigma_x = self.compute_mean_and_covariance(embeddings_ground_truth)\n",
    "        mu_y, sigma_y = self.compute_mean_and_covariance(embeddings_retrieved)\n",
    "\n",
    "        # Calculate the term: ||mu_x - mu_y||^2\n",
    "        diff_mu = mu_x - mu_y\n",
    "        mu_term = torch.sum(diff_mu ** 2)\n",
    "\n",
    "        # Add epsilon to the covariance matrices to ensure they are positive semi-definite\n",
    "        sigma_x += epsilon * torch.eye(sigma_x.size(0), device=sigma_x.device)\n",
    "        sigma_y += epsilon * torch.eye(sigma_y.size(0), device=sigma_y.device)\n",
    "\n",
    "        # Perform Cholesky decomposition to compute the matrix square root of covariance matrices\n",
    "        try:\n",
    "            sigma_x_sqrt = torch.linalg.cholesky(sigma_x)\n",
    "            sigma_y_sqrt = torch.linalg.cholesky(sigma_y)\n",
    "        except RuntimeError:\n",
    "            raise ValueError(\"Covariance matrices are not positive semi-definite, even with regularization\")\n",
    "            return float('inf')\n",
    "\n",
    "        # Calculate the term: Tr(sigma_x + sigma_y - 2(sigma_x^0.5 * sigma_y * sigma_x^0.5)^0.5)\n",
    "        term = torch.trace(sigma_x + sigma_y - 2 * torch.matmul(sigma_x_sqrt, torch.matmul(sigma_y_sqrt, sigma_x_sqrt.T)))\n",
    "\n",
    "        # The FAD is the sum of the two terms\n",
    "        fad = mu_term + term\n",
    "        return fad.item()\n",
    "\n",
    "    def top_k_recall(self, similarity_matrix, k):\n",
    "        \"\"\"\n",
    "        Compute top-k recall from a similarity matrix (vectorized version).\n",
    "\n",
    "        similarity_matrix: Tensor of shape (batch_size, batch_size), where\n",
    "                           diagonal elements represent positive pairs.\n",
    "        k: The value of k for top-k recall.\n",
    "\n",
    "        Returns:\n",
    "            recall: The top-k recall (mean recall across all examples).\n",
    "        \"\"\"\n",
    "        batch_size = similarity_matrix.size(0)\n",
    "\n",
    "        # Get the indices of the top-k most similar items for each row (video/audio example)\n",
    "        _, top_k_indices = torch.topk(similarity_matrix, k, dim=1)\n",
    "\n",
    "        # Create a tensor for diagonal indices (i, i) for each row\n",
    "        diagonal_indices = torch.arange(batch_size, device=similarity_matrix.device)\n",
    "\n",
    "        # Check if the diagonal index of each row is in the top-K indices of that row\n",
    "        # `top_k_indices` is of shape (batch_size, k)\n",
    "        is_true_positive_in_top_k = (top_k_indices == diagonal_indices.unsqueeze(1))\n",
    "\n",
    "        # Calculate recall: for each row, check if the diagonal index is in the top-K\n",
    "        recall_per_row = is_true_positive_in_top_k.any(dim=1).float()\n",
    "\n",
    "        # Return the mean recall across all examples (rows)\n",
    "        return recall_per_row.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "9d218a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_evaluations(video_model, audio_model, batch_size, window_size, path, epoch, ks=[1, 5]):\n",
    "\n",
    "    metrics = Eval()\n",
    "    testloader = get_dataloader(path, batch_size=batch_size, shuffle=True, method='video', window_size=window_size)\n",
    "    audio_model.eval()\n",
    "    video_model.eval()\n",
    "\n",
    "    recalls = []\n",
    "    fads = []\n",
    "    av_aligns = []\n",
    "\n",
    "    \n",
    "    for video_batch, audio_batch, flow_ranks in testloader:\n",
    "        batch_aud_embeddings, batch_vid_embeddings = get_batch_embeddings(video_model, audio_model, video_batch, audio_batch)\n",
    "\n",
    "        # These were in (#segments*batchsize, 256)\n",
    "        # Now they are in (batchsize, 256 * #segments)\n",
    "        batch_vid_embeddings = batch_vid_embeddings.reshape(batch_size, -1)\n",
    "        batch_aud_embeddings = batch_aud_embeddings.reshape(batch_size, -1)\n",
    "\n",
    "        # we are going to do a naive cosine similarity based retrieval strategy\n",
    "        similarity_matrix = torch.matmul(batch_vid_embeddings, batch_aud_embeddings.T)\n",
    "\n",
    "        # Get the most similar audio embeddings for each video\n",
    "        _, most_similar_indices = torch.max(similarity_matrix, dim=1)\n",
    "\n",
    "        # recall@k\n",
    "        recall = [metrics.top_k_recall(similarity_matrix, k) for k in ks]\n",
    "\n",
    "        retrieved_audio_embeddings = batch_aud_embeddings[most_similar_indices]\n",
    "        fad = metrics.calculate_frechet_audio_distance(batch_aud_embeddings, retrieved_audio_embeddings)\n",
    "        av_align = metrics.compute_av_align_score(batch_vid_embeddings, retrieved_audio_embeddings, prominence_threshold=0.1)\n",
    "\n",
    "\n",
    "        recalls.append(recall)\n",
    "        fads.append(fad)\n",
    "        av_aligns.append(av_align)\n",
    "        break\n",
    "    mean_recalls = np.mean(recalls, axis=0)\n",
    "    print(f'Mean Recall@{ks}: {mean_recalls}')\n",
    "    print(f'Mean AV-Align: {np.mean(av_aligns)}')\n",
    "    print(f'Mean AV-Align: {np.mean(fads)}')\n",
    "    \n",
    "    tmp = pd.DataFrame({'epoch':epoch,\n",
    "       'fad':np.mean(av_aligns),\n",
    "       'av_align':np.mean(fads)\n",
    "      }, index=[0])\n",
    "\n",
    "    mean_recalls = np.mean(recalls, axis=0)\n",
    "    for i, k in enumerate(ks):\n",
    "        tmp[f'recall@{k}'] = mean_recalls[i]\n",
    "        \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2674ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "e0511c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({'epoch':1,\n",
    "       'fad':np.mean(av_aligns),\n",
    "       'av_align':np.mean(fads)\n",
    "      }, index=[0])\n",
    "mean_recalls = np.mean(recalls, axis=0)\n",
    "for i, k in enumerate(ks):\n",
    "    tmp[f'recall@{k}'] = mean_recalls[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "f2def582",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "c64d8787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Recall@[1, 5]: [0.16666667 0.8333333 ]\n",
      "Mean AV-Align: 0.07398568019093078\n",
      "Mean AV-Align: 0.6593323945999146\n"
     ]
    }
   ],
   "source": [
    "tmp = compute_evaluations(video_model, audio_model, 6, window_size, path, 1, ks=[1, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "a63c6425",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "77fa22a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>fad</th>\n",
       "      <th>av_align</th>\n",
       "      <th>recall@1</th>\n",
       "      <th>recall@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.073986</td>\n",
       "      <td>0.659332</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.073986</td>\n",
       "      <td>0.659332</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.073986</td>\n",
       "      <td>0.659332</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch       fad  av_align  recall@1  recall@5\n",
       "0      1  0.073986  0.659332  0.166667  0.833333\n",
       "0      1  0.073986  0.659332  0.166667  0.833333\n",
       "0      1  0.073986  0.659332  0.166667  0.833333"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "a39494a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[632], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mOFVMNET\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mOFProcessor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpticalFlowProcessor\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from .OFVMNET.OFProcessor import OpticalFlowProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2717b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
